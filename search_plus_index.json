{"./":{"url":"./","title":"Introduction","keywords":"","body":" ğŸŒ ğŸ“ƒ NLG-Chat (åŸ Langchain-ChatGLM) åŸºäº ChatGLM ç­‰å¤§è¯­è¨€æ¨¡å‹ä¸ Langchain ç­‰åº”ç”¨æ¡†æ¶å®ç°ï¼Œå¼€æºã€å¯ç¦»çº¿éƒ¨ç½²çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)å¤§æ¨¡å‹çŸ¥è¯†åº“é¡¹ç›®ã€‚ ç›®å½• ä»‹ç» è§£å†³çš„ç—›ç‚¹ å¿«é€Ÿä¸Šæ‰‹ 1. ç¯å¢ƒé…ç½® 2. æ¨¡å‹ä¸‹è½½ 3. åˆå§‹åŒ–çŸ¥è¯†åº“å’Œé…ç½®æ–‡ä»¶ 4. ä¸€é”®å¯åŠ¨ 5. å¯åŠ¨ç•Œé¢ç¤ºä¾‹ è”ç³»æˆ‘ä»¬ ä»‹ç» ğŸ¤–ï¸ ä¸€ç§åˆ©ç”¨ langchain æ€æƒ³å®ç°çš„åŸºäºæœ¬åœ°çŸ¥è¯†åº“çš„é—®ç­”åº”ç”¨ï¼Œç›®æ ‡æœŸæœ›å»ºç«‹ä¸€å¥—å¯¹ä¸­æ–‡åœºæ™¯ä¸å¼€æºæ¨¡å‹æ”¯æŒå‹å¥½ã€å¯ç¦»çº¿è¿è¡Œçš„çŸ¥è¯†åº“é—®ç­”è§£å†³æ–¹æ¡ˆã€‚ ğŸ’¡ å— GanymedeNil çš„é¡¹ç›® document.ai å’Œ AlexZhangji åˆ›å»ºçš„ ChatGLM-6B Pull Request å¯å‘ï¼Œå»ºç«‹äº†å…¨æµç¨‹å¯ä½¿ç”¨å¼€æºæ¨¡å‹å®ç°çš„æœ¬åœ°çŸ¥è¯†åº“é—®ç­”åº”ç”¨ã€‚æœ¬é¡¹ç›®çš„æœ€æ–°ç‰ˆæœ¬ä¸­é€šè¿‡ä½¿ç”¨ FastChat æ¥å…¥ Vicuna, Alpaca, LLaMA, Koala, RWKV ç­‰æ¨¡å‹ï¼Œä¾æ‰˜äº langchain æ¡†æ¶æ”¯æŒé€šè¿‡åŸºäº FastAPI æä¾›çš„ API è°ƒç”¨æœåŠ¡ï¼Œæˆ–ä½¿ç”¨åŸºäº Streamlit çš„ WebUI è¿›è¡Œæ“ä½œã€‚ âœ… ä¾æ‰˜äºæœ¬é¡¹ç›®æ”¯æŒçš„å¼€æº LLM ä¸ Embedding æ¨¡å‹ï¼Œæœ¬é¡¹ç›®å¯å®ç°å…¨éƒ¨ä½¿ç”¨å¼€æºæ¨¡å‹ç¦»çº¿ç§æœ‰éƒ¨ç½²ã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ¬é¡¹ç›®ä¹Ÿæ”¯æŒ OpenAI GPT API çš„è°ƒç”¨ï¼Œå¹¶å°†åœ¨åç»­æŒç»­æ‰©å……å¯¹å„ç±»æ¨¡å‹åŠæ¨¡å‹ API çš„æ¥å…¥ã€‚ â›“ï¸ æœ¬é¡¹ç›®å®ç°åŸç†å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œè¿‡ç¨‹åŒ…æ‹¬åŠ è½½æ–‡ä»¶ -> è¯»å–æ–‡æœ¬ -> æ–‡æœ¬åˆ†å‰² -> æ–‡æœ¬å‘é‡åŒ– -> é—®å¥å‘é‡åŒ– -> åœ¨æ–‡æœ¬å‘é‡ä¸­åŒ¹é…å‡ºä¸é—®å¥å‘é‡æœ€ç›¸ä¼¼çš„ top kä¸ª -> åŒ¹é…å‡ºçš„æ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡å’Œé—®é¢˜ä¸€èµ·æ·»åŠ åˆ° promptä¸­ -> æäº¤ç»™ LLMç”Ÿæˆå›ç­”ã€‚ ğŸ“º åŸç†ä»‹ç»è§†é¢‘ ä»æ–‡æ¡£å¤„ç†è§’åº¦æ¥çœ‹ï¼Œå®ç°æµç¨‹å¦‚ä¸‹ï¼š ğŸš© æœ¬é¡¹ç›®æœªæ¶‰åŠå¾®è°ƒã€è®­ç»ƒè¿‡ç¨‹ï¼Œä½†å¯åˆ©ç”¨å¾®è°ƒæˆ–è®­ç»ƒå¯¹æœ¬é¡¹ç›®æ•ˆæœè¿›è¡Œä¼˜åŒ–ã€‚ ğŸŒ AutoDL é•œåƒ ä¸­ v11 ç‰ˆæœ¬æ‰€ä½¿ç”¨ä»£ç å·²æ›´æ–°è‡³æœ¬é¡¹ç›® v0.2.7 ç‰ˆæœ¬ã€‚ ğŸ³ Docker é•œåƒ å·²ç»æ›´æ–°åˆ° 0.2.7 ç‰ˆæœ¬ã€‚ ğŸŒ² ä¸€è¡Œå‘½ä»¤è¿è¡Œ Docker ï¼š docker run -d --gpus all -p 80:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.7 ğŸ§© æœ¬é¡¹ç›®æœ‰ä¸€ä¸ªéå¸¸å®Œæ•´çš„Wiki ï¼Œ READMEåªæ˜¯ä¸€ä¸ªç®€å•çš„ä»‹ç»ï¼Œä»…ä»…æ˜¯å…¥é—¨æ•™ç¨‹ï¼Œèƒ½å¤ŸåŸºç¡€è¿è¡Œã€‚ å¦‚æœä½ æƒ³è¦æ›´æ·±å…¥çš„äº†è§£æœ¬é¡¹ç›®ï¼Œæˆ–è€…æƒ³å¯¹æœ¬é¡¹ç›®åšå‡ºè´¡çŒ®ã€‚è¯·ç§»æ­¥ Wiki ç•Œé¢ è§£å†³çš„ç—›ç‚¹ è¯¥é¡¹ç›®æ˜¯ä¸€ä¸ªå¯ä»¥å®ç° å®Œå…¨æœ¬åœ°åŒ–æ¨ç†çš„çŸ¥è¯†åº“å¢å¼ºæ–¹æ¡ˆ, é‡ç‚¹è§£å†³æ•°æ®å®‰å…¨ä¿æŠ¤ï¼Œç§åŸŸåŒ–éƒ¨ç½²çš„ä¼ä¸šç—›ç‚¹ã€‚ æœ¬å¼€æºæ–¹æ¡ˆé‡‡ç”¨Apache Licenseï¼Œå¯ä»¥å…è´¹å•†ç”¨ï¼Œæ— éœ€ä»˜è´¹ã€‚ æˆ‘ä»¬æ”¯æŒå¸‚é¢ä¸Šä¸»æµçš„æœ¬åœ°å¤§è¯­è¨€æ¨¡å‹å’ŒEmbeddingæ¨¡å‹ï¼Œæ”¯æŒå¼€æºçš„æœ¬åœ°å‘é‡æ•°æ®åº“ã€‚ æ”¯æŒåˆ—è¡¨è¯¦è§Wiki å¿«é€Ÿä¸Šæ‰‹ 1. ç¯å¢ƒé…ç½® é¦–å…ˆï¼Œç¡®ä¿ä½ çš„æœºå™¨å®‰è£…äº† Python 3.8 - 3.10$ python --version Python 3.10.12 æ¥ç€ï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒï¼Œå¹¶åœ¨è™šæ‹Ÿç¯å¢ƒå†…å®‰è£…é¡¹ç›®çš„ä¾èµ–# æ‹‰å–ä»“åº“ $ git clone https://github.com/6llm/NLG-Chat.git # è¿›å…¥ç›®å½• $ cd NLG-Chat # å®‰è£…å…¨éƒ¨ä¾èµ– pip install -r requirements.txt pip install -r requirements_api.txt pip install -r requirements_webui.txt $ pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple $ pip install -r requirements_api.txt -i https://pypi.tuna.tsinghua.edu.cn/simple $ pip install -r requirements_webui.txt -i https://pypi.tuna.tsinghua.edu.cn/simple é»˜è®¤ä¾èµ–åŒ…æ‹¬åŸºæœ¬è¿è¡Œç¯å¢ƒï¼ˆFAISSå‘é‡åº“ï¼‰ã€‚ å¦‚æœè¦ä½¿ç”¨ milvus/pg_vector ç­‰å‘é‡åº“ï¼Œè¯·å°† requirements.txt ä¸­ç›¸åº”ä¾èµ–å–æ¶ˆæ³¨é‡Šå†å®‰è£…ã€‚ 2ï¼Œ æ¨¡å‹ä¸‹è½½ å¦‚éœ€åœ¨æœ¬åœ°æˆ–ç¦»çº¿ç¯å¢ƒä¸‹è¿è¡Œæœ¬é¡¹ç›®ï¼Œéœ€è¦é¦–å…ˆå°†é¡¹ç›®æ‰€éœ€çš„æ¨¡å‹ä¸‹è½½è‡³æœ¬åœ°ï¼Œé€šå¸¸å¼€æº LLM ä¸ Embedding æ¨¡å‹å¯ä»¥ä» HuggingFace ä¸‹è½½ã€‚ ä»¥æœ¬é¡¹ç›®ä¸­é»˜è®¤ä½¿ç”¨çš„ LLM æ¨¡å‹ THUDM/ChatGLM3-6B ä¸ Embedding æ¨¡å‹ BAAI/bge-large-zh ä¸ºä¾‹ï¼š ä¸‹è½½æ¨¡å‹éœ€è¦å…ˆå®‰è£… Git LFSï¼Œç„¶åè¿è¡Œ $ git lfs install $ git clone https://huggingface.co/THUDM/chatglm3-6b $ git clone https://huggingface.co/BAAI/bge-large-zh 3. åˆå§‹åŒ–çŸ¥è¯†åº“å’Œé…ç½®æ–‡ä»¶ æŒ‰ç…§ä¸‹åˆ—æ–¹å¼åˆå§‹åŒ–è‡ªå·±çš„çŸ¥è¯†åº“å’Œç®€å•çš„å¤åˆ¶é…ç½®æ–‡ä»¶ $ python copy_config_example.py $ python init_database.py --recreate-vs 4. ä¸€é”®å¯åŠ¨ æŒ‰ç…§ä»¥ä¸‹å‘½ä»¤å¯åŠ¨é¡¹ç›® $ python startup.py -a 5. å¯åŠ¨ç•Œé¢ç¤ºä¾‹ å¦‚æœæ­£å¸¸å¯åŠ¨ï¼Œä½ å°†èƒ½çœ‹åˆ°ä»¥ä¸‹ç•Œé¢ FastAPI Docs ç•Œé¢ Web UI å¯åŠ¨ç•Œé¢ç¤ºä¾‹ï¼š Web UI å¯¹è¯ç•Œé¢ï¼š Web UI çŸ¥è¯†åº“ç®¡ç†é¡µé¢ï¼š æ³¨æ„ ä»¥ä¸Šæ–¹å¼åªæ˜¯ä¸ºäº†å¿«é€Ÿä¸Šæ‰‹ï¼Œå¦‚æœéœ€è¦æ›´å¤šçš„åŠŸèƒ½å’Œè‡ªå®šä¹‰å¯åŠ¨æ–¹å¼ ï¼Œè¯·å‚è€ƒWiki é¡¹ç›®é‡Œç¨‹ç¢‘ è”ç³»æˆ‘ä»¬ Telegram é¡¹ç›®äº¤æµç¾¤ ğŸ‰ NLG-Chat é¡¹ç›®å¾®ä¿¡äº¤æµç¾¤ï¼Œå¦‚æœä½ ä¹Ÿå¯¹æœ¬é¡¹ç›®æ„Ÿå…´è¶£ï¼Œæ¬¢è¿åŠ å…¥ç¾¤èŠå‚ä¸è®¨è®ºäº¤æµã€‚ å…¬ä¼—å· ğŸ‰ NLG-Chat é¡¹ç›®å®˜æ–¹å…¬ä¼—å·ï¼Œæ¬¢è¿æ‰«ç å…³æ³¨ã€‚ https://docs.streamlit.io/library/2api-reference/chat Â© snowdreams1006 all right reservedï¼Œpowered by Gitbookæ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š 2024-05-18 00:25:38 "},"LLM/æ”¯æŒåˆ—è¡¨.html":{"url":"LLM/æ”¯æŒåˆ—è¡¨.html","title":"æ”¯æŒåˆ—è¡¨","keywords":"","body":"LLM æ¨¡å‹æ”¯æŒåˆ—è¡¨ æœ¬åœ°æ¨¡å‹ æœ¬åœ° LLM æ¨¡å‹æ¥å…¥åŸºäº FastChat å®ç°ï¼Œæ”¯æŒæ¨¡å‹å¦‚ä¸‹ï¼š ChatGLM å…¨ç³»ç±»å¯¹è¯æ¨¡å‹ Orion å…¨ç³»åˆ—å¯¹è¯æ¨¡å‹ï¼Œå¿…é¡»å®‰è£…flash-attn æ‰èƒ½ä½¿ç”¨ Qwen å…¨ç³»åˆ—å¯¹è¯æ¨¡å‹ internlm å…¨ç³»åˆ—å¯¹è¯æ¨¡å‹ Baichuan å…¨ç³»åˆ—å¯¹è¯æ¨¡å‹ï¼Œå¿…é¡»é™çº§transformeræ‰èƒ½ä½¿ç”¨ llama å…¨ç³»åˆ—å¯¹è¯æ¨¡å‹ Vicuna å…¨ç³»åˆ—å¯¹è¯æ¨¡å‹ mistral å…¨ç³»åˆ—å¯¹è¯æ¨¡å‹ vivo-ai/BlueLM-7B-Chat 01-ai/Yi-34B-Chat BlinkDL/RWKV-4-Raven camel-ai/CAMEL-13B-Combined-Data databricks/dolly-v2-12b FreedomIntelligence/phoenix-inst-chat-7b h2oai/h2ogpt-gm-oasst1-en-2048-open-llama-7b lcw99/polyglot-ko-12.8b-chang-instruct-chat lmsys/fastchat-t5-3b-v1.0 mosaicml/mpt-7b-chat Neutralzz/BiLLa-7B-SFT nomic-ai/gpt4all-13b-snoozy NousResearch/Nous-Hermes-13b openaccess-ai-collective/manticore-13b-chat-pyg OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5 project-baize/baize-v2-7b Salesforce/codet5p-6b StabilityAI/stablelm-tuned-alpha-7b tiiuae/falcon-40b timdettmers/guanaco-33b-merged togethercomputer/RedPajama-INCITE-7B-Chat WizardLM/WizardLM-13B-V1.0 WizardLM/WizardCoder-15B-V1.0 HuggingFaceH4/starchat-beta FlagAlpha/Llama2-Chinese-13b-Chat and others BAAI/AquilaChat-7B all models of OpenOrca Spicyboros airoboros 2.2 VMware's OpenLLaMa OpenInstruct ä»»ä½• EleutherAI çš„ pythia æ¨¡å‹ï¼Œå¦‚ pythia-6.9b åœ¨ä»¥ä¸Šæ¨¡å‹åŸºç¡€ä¸Šè®­ç»ƒçš„ä»»ä½• Peft é€‚é…å™¨ã€‚ä¸ºäº†æ¿€æ´»ï¼Œæ¨¡å‹è·¯å¾„ä¸­å¿…é¡»æœ‰ peft ã€‚æ³¨æ„ï¼šå¦‚æœåŠ è½½å¤šä¸ªpeftæ¨¡å‹ï¼Œä½ å¯ä»¥é€šè¿‡åœ¨ä»»ä½•æ¨¡å‹å·¥ä½œå™¨ä¸­è®¾ç½®ç¯å¢ƒå˜é‡ PEFT_SHARE_BASE_WEIGHTS=true æ¥ä½¿å®ƒä»¬å…±äº«åŸºç¡€æ¨¡å‹çš„æƒé‡ã€‚ ä»¥ä¸Šæ¨¡å‹æ”¯æŒåˆ—è¡¨å¯èƒ½éš FastChat æ›´æ–°è€ŒæŒç»­æ›´æ–°ï¼Œå¯å‚è€ƒ FastChat å·²æ”¯æŒæ¨¡å‹åˆ—è¡¨ã€‚ è”ç½‘æ¨¡å‹ æ”¯æŒçš„è”ç½‘æ¨¡å‹ æ™ºè°±AI ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼Œä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œå°½æƒ…æœŸå¾…0.3.x é˜¿é‡Œäº‘é€šä¹‰åƒé—® ç™¾å· ChatGPT Gimini Azure OpenAI MiniMax è®¯é£æ˜Ÿç« ç™¾åº¦åƒå¸† å­—èŠ‚ç«å±±æ–¹èˆŸ Embedding æ¨¡å‹æ”¯æŒåˆ—è¡¨ æœ¬åœ°æ¨¡å‹ æœ¬é¡¹ç›®æ”¯æŒè°ƒç”¨ HuggingFace ä¸­çš„ Embedding æ¨¡å‹ï¼Œå·²æ”¯æŒçš„ Embedding æ¨¡å‹å¦‚ä¸‹ï¼š MokaAIç³»åˆ—åµŒå…¥æ¨¡å‹ moka-ai/m3e-small moka-ai/m3e-base moka-ai/m3e-large BAAIç³»åˆ—åµŒå…¥æ¨¡å‹ BAAI/bge-small-zh BAAI/bge-base-zh BAAI/bge-large-zh BAAI/bge-small-zh-v1.5 BAAI/bge-base-zh-v1.5 BAAI/bge-large-zh-v1.5 BAAI/bge-large-zh-noinstruct BAAI/bge-reranker-large BAAI/bge-reranker-base text2vecç³»åˆ—åµŒå…¥æ¨¡å‹ shibing624/text2vec-base-chinese-sentence shibing624/text2vec-base-chinese-paraphrase shibing624/text2vec-base-multilingual shibing624/text2vec-base-chinese shibing624/text2vec-bge-large-chinese GanymedeNil/text2vec-large-chinese å…¶ä»–æ¨¡å‹ sensenova/piccolo-base-zh sensenova/piccolo-large-zh nghuyong/ernie-3.0-nano-zh nghuyong/ernie-3.0-base-zh è¾¾æ‘©é™¢ç³»åˆ—åµŒå…¥æ¨¡å‹ damo/nlp_gte_sentence-embedding_chinese-large è”ç½‘æ¨¡å‹ é™¤æœ¬åœ°æ¨¡å‹å¤–ï¼Œæœ¬é¡¹ç›®ä¹Ÿæ”¯æŒç›´æ¥æ¥å…¥ OpenAIçš„åœ¨çº¿åµŒå…¥æ¨¡å‹ã€‚ æ”¯æŒçš„è”ç½‘æ¨¡å‹ OpenAI/text-embedding-ada-002 æ™ºè°±AI MiniMax ç™¾åº¦åƒå¸† é˜¿é‡Œäº‘é€šä¹‰åƒé—® åˆ†è¯å™¨æ”¯æŒåˆ—è¡¨ Langchain ä¸­çš„åˆ†è¯å™¨ æœ¬é¡¹ç›®æ”¯æŒè°ƒç”¨ Langchain çš„ Text Splitter åˆ†è¯å™¨ä»¥åŠåŸºäºæ­¤æ”¹è¿›çš„è‡ªå®šä¹‰åˆ†è¯å™¨ï¼Œå·²æ”¯æŒçš„ Text Splitter ç±»å‹å¦‚ä¸‹ï¼š CharacterTextSplitter LatexTextSplitter MarkdownHeaderTextSplitter MarkdownTextSplitter NLTKTextSplitter PythonCodeTextSplitter RecursiveCharacterTextSplitter SentenceTransformersTokenTextSplitter SpacyTextSplitter è‡ªå®šä¹‰åˆ†è¯å™¨ å·²ç»æ”¯æŒçš„å®šåˆ¶åˆ†è¯å™¨å¦‚ä¸‹ï¼š AliTextSplitter ChineseRecursiveTextSplitter ChineseTextSplitter å‘é‡æ•°æ®åº“æ”¯æŒåˆ—è¡¨ æœ¬åœ°å‘é‡æ•°æ®åº“ ç›®å‰æ”¯æŒçš„æœ¬åœ°å‘é‡æ•°æ®åº“åˆ—è¡¨å¦‚ä¸‹ï¼š FAISS Milvus PGVector Chroma è”ç½‘å‘é‡æ•°æ®åº“ Zilliz å·¥å…·æ”¯æŒåˆ—è¡¨ Langchainå·¥å…· Shell å·¥å…·ï¼Œç”¨äºæ¨¡æ‹Ÿå½“å‰çš„Linux Shellç¯å¢ƒ Youtube å·¥å…·ï¼Œç”¨äºæœç´¢Youtubeçš„ç›¸å…³è§†é¢‘é“¾æ¥ Wolfram å·¥å…·ï¼Œç”¨Wolframæ¥å®ç°æ•°å­¦è®¡ç®—ç­‰ å…¶ä»–Langchainè‡ªå¸¦çš„å·¥å…·ä¹Ÿå¯ä»¥æŒ‰ç…§ä¸Šè¿°ä¸‰ä¸ªå·¥å…·çš„æ–¹å¼æ¥è‡ªå·±å®ç° æœ¬åœ°å·¥å…· ç¿»è¯‘å·¥å…·ï¼Œå®ç°å¯¹è¾“å…¥çš„ä»»æ„è¯­è¨€ç¿»è¯‘ã€‚ æ•°å­¦å·¥å…·ï¼Œä½¿ç”¨LLMMathChain å®ç°æ•°å­¦è®¡ç®—ã€‚ é«˜çº§çŸ¥è¯†åº“å·¥å…·ï¼Œæ™ºèƒ½é€‰æ‹©è°ƒç”¨å¤šä¸ªæˆ–è€…å•ä¸ªçŸ¥è¯†åº“å¹¶æŸ¥è¯¢å†…å®¹ã€‚ è¿›é˜¶çŸ¥è¯†åº“å·¥å…·ï¼Œæ™ºèƒ½é€‰æ‹©è°ƒç”¨ä¸€ä¸ªæœ€ç›¸è¿‘çš„çŸ¥è¯†åº“å¹¶æŸ¥è¯¢å†…å®¹ã€‚ åŸºç¡€çŸ¥è¯†åº“å·¥å…·ï¼Œé€‰æ‹©æŒ‡å®šçš„ä¸€ä¸ªçŸ¥è¯†åº“å¹¶å›ç­”ã€‚ è”ç½‘å·¥å…· å¤©æ°”å·¥å…·ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„LLMWetherChainå®ç°å¤©æ°”æŸ¥è¯¢ï¼Œè°ƒç”¨å’Œé£å¤©æ°”APIã€‚ æœç´¢å·¥å…·ï¼Œä½¿ç”¨æˆ‘ä»¬çš„æœç´¢APIæ¥å®ç°æœç´¢å¹¶æ¦‚æ‹¬å†…å®¹ã€‚ æˆ‘ä»¬æœŸå¾…å¼€å‘è€…å…±äº«æ›´å¤šçš„å·¥å…·ï¼Œå¸®åŠ©é¡¹ç›®ç”Ÿæ€å®Œå–„ Â© snowdreams1006 all right reservedï¼Œpowered by Gitbookæ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š 2024-05-17 23:23:32 "},"LLM/å¼€å‘ç¯å¢ƒéƒ¨ç½².html":{"url":"LLM/å¼€å‘ç¯å¢ƒéƒ¨ç½².html","title":"å¼€å‘ç¯å¢ƒéƒ¨ç½²","keywords":"","body":"è½¯ä»¶è¦æ±‚ è¦é¡ºåˆ©è¿è¡Œæœ¬ä»£ç ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹ç³»ç»Ÿè¦æ±‚è¿›è¡Œé…ç½® å·²ç»æµ‹è¯•è¿‡çš„ç³»ç»Ÿ Linux Ubuntu 22.04.5 kernel version 6.7 å…¶ä»–ç³»ç»Ÿå¯èƒ½å‡ºç°ç³»ç»Ÿå…¼å®¹æ€§é—®é¢˜ã€‚ æœ€ä½è¦æ±‚ è¯¥è¦æ±‚ä»…é’ˆå¯¹æ ‡å‡†æ¨¡å¼ï¼Œè½»é‡æ¨¡å¼ä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼Œä¸éœ€è¦å®‰è£…torchç­‰åº“ï¼Œä¹Ÿä¸éœ€è¦æ˜¾å¡å³å¯è¿è¡Œã€‚ Python ç‰ˆæœ¬: >= 3.8(å¾ˆä¸ç¨³å®š), CUDA ç‰ˆæœ¬: >= 12.1 æ¨èè¦æ±‚ å¼€å‘è€…åœ¨ä»¥ä¸‹ç¯å¢ƒä¸‹è¿›è¡Œä»£ç è°ƒè¯•ï¼Œåœ¨è¯¥ç¯å¢ƒä¸‹èƒ½å¤Ÿé¿å…æœ€å¤šç¯å¢ƒé—®é¢˜ã€‚ Python ç‰ˆæœ¬ == 3.11.7 CUDA ç‰ˆæœ¬: == 12.1 ç¡¬ä»¶è¦æ±‚ æœ¬æ¡†æ¶ä½¿ç”¨ fschaté©±åŠ¨ï¼Œç»Ÿä¸€ä½¿ç”¨ huggingfaceè¿›è¡Œæ¨ç†ï¼Œå…¶ä»–æ¨ç†æ–¹å¼(å¦‚ llama-cppï¼ŒTensorRTåŠ é€Ÿå¼•æ“ å»ºè®®é€šè¿‡æ¨ç†å¼•æ“ä»¥ API å½¢å¼æ¥å…¥æˆ‘ä»¬çš„æ¡†æ¶)ã€‚ åŒæ—¶, æˆ‘ä»¬æ²¡æœ‰å¯¹ Int4 æ¨¡å‹è¿›è¡Œé€‚é…ï¼Œä¸ä¿è¯Int4æ¨¡å‹èƒ½å¤Ÿæ­£å¸¸è¿è¡Œã€‚å› æ­¤ï¼Œé‡åŒ–ç‰ˆæœ¬æš‚æ—¶éœ€è¦ç”±å¼€å‘è€…è‡ªè¡Œé€‚é…, æˆ‘ä»¬å¯èƒ½åœ¨æœªæ¥æ”¾ã€‚ å¦‚æœæƒ³è¦é¡ºåˆ©åœ¨GPUè¿è¡Œæœ¬åœ°æ¨¡å‹çš„ FP16 ç‰ˆæœ¬ï¼Œä½ è‡³å°‘éœ€è¦ä»¥ä¸‹çš„ç¡¬ä»¶é…ç½®ï¼Œæ¥ä¿è¯åœ¨æˆ‘ä»¬æ¡†æ¶ä¸‹èƒ½å¤Ÿå®ç° ç¨³å®šè¿ç»­å¯¹è¯ ChatGLM3-6B & LLaMA-7B-Chat ç­‰ 7Bæ¨¡å‹ æœ€ä½æ˜¾å­˜è¦æ±‚: 14GB æ¨èæ˜¾å¡: RTX 4080 Qwen-14B-Chat ç­‰ 14Bæ¨¡å‹ æœ€ä½æ˜¾å­˜è¦æ±‚: 30GB æ¨èæ˜¾å¡: V100 Yi-34B-Chat ç­‰ 34Bæ¨¡å‹ æœ€ä½æ˜¾å­˜è¦æ±‚: 69GB æ¨èæ˜¾å¡: A100 Qwen-72B-Chat ç­‰ 72Bæ¨¡å‹ æœ€ä½æ˜¾å­˜è¦æ±‚: 145GB æ¨èæ˜¾å¡ï¼šå¤šå¡ A100 ä»¥ä¸Š ä¸€ç§ç®€å•çš„ä¼°ç®—æ–¹å¼ä¸ºï¼š FP16: æ˜¾å­˜å ç”¨(GB) = æ¨¡å‹é‡çº§ x 2 Int4: æ˜¾å­˜å ç”¨(GB) = æ¨¡å‹é‡çº§ x 0.75 ä»¥ä¸Šæ•°æ®ä»…ä¸ºä¼°ç®—ï¼Œå®é™…æƒ…å†µä»¥ nvidia-smi å ç”¨ä¸ºå‡†ã€‚ è¯·æ³¨æ„ï¼Œå¦‚æœä½¿ç”¨æœ€ä½é…ç½®ï¼Œä»…èƒ½ä¿è¯ä»£ç èƒ½å¤Ÿè¿è¡Œï¼Œä½†è¿è¡Œé€Ÿåº¦è¾ƒæ…¢ï¼Œä½“éªŒä¸ä½³ã€‚ åŒæ—¶ï¼ŒEmbedding æ¨¡å‹å°†ä¼šå ç”¨ 1-2G çš„æ˜¾å­˜ï¼Œå†å²è®°å½•æœ€å¤šä¼šå ç”¨ æ•°GB çš„æ˜¾å­˜ï¼Œå› æ­¤ï¼Œéœ€è¦å¤šå†—ä½™ä¸€äº›æ˜¾å­˜ã€‚ å†…å­˜æœ€ä½è¦æ±‚: å†…å­˜è¦æ±‚è‡³å°‘åº”è¯¥æ¯”æ¨¡å‹è¿è¡Œçš„æ˜¾å­˜å¤§ã€‚ ä¾‹å¦‚ï¼Œè¿è¡ŒChatGLM3-6B FP16 æ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨13Gï¼Œæ¨èä½¿ç”¨16Gä»¥ä¸Šå†…å­˜ã€‚ éƒ¨åˆ†æµ‹è¯•ç”¨æœºé…ç½®å‚è€ƒï¼Œåœ¨ä»¥ä¸‹æœºå™¨ä¸‹å¼€å‘ç»„æˆå‘˜å·²ç»è¿›è¡ŒåŸç”Ÿæ¨¡æ‹Ÿæµ‹è¯•ï¼ˆåˆ›å»ºæ–°ç¯å¢ƒå¹¶æ ¹æ®è¦æ±‚ä¸‹è½½åè¿è¡Œï¼‰ï¼Œç¡®ä¿èƒ½æµç•…è¿è¡Œå…¨éƒ¨åŠŸèƒ½çš„ä»£ç æ¡†æ¶ã€‚ æœåŠ¡å™¨å¤„ç†å™¨: IntelÂ® XeonÂ® Platinum 8558P Processor (260M Cache, 2.7 GHz) å†…å­˜: 4 TB æ˜¾å¡ç»„: NVIDIA H800 SXM5 80GB x 8 ç¡¬ç›˜: 6 PB æ“ä½œç³»ç»Ÿ: Ubuntu 22.04 LTS,Linux kernel 5.15.0-60-generic æ˜¾å¡é©±åŠ¨ç‰ˆæœ¬: 535.129.03 Cudaç‰ˆæœ¬: 12.1 Pythonç‰ˆæœ¬: 3.11.7 ç½‘ç»œIPåœ°å€ï¼šç¾å›½ï¼Œæ´›æ‰çŸ¶ ä¸ªäººPCå¤„ç†å™¨: IntelÂ® Coreâ„¢ i9 processor 14900K å†…å­˜: 256 GB DDR5 æ˜¾å¡ç»„: NVIDIA RTX4090 X 1 / NVIDIA RTXA6000 X 1 ç¡¬ç›˜: 1 TB æ“ä½œç³»ç»Ÿ: Ubuntu 22.04 LTS / Arch Linux, Linux Kernel 6.6.7 æ˜¾å¡é©±åŠ¨ç‰ˆæœ¬: 545.29.06 Cudaç‰ˆæœ¬: 12.3 Update 1 Pythonç‰ˆæœ¬: 3.11.7 ç½‘ç»œIPåœ°å€ï¼šä¸­å›½ï¼Œä¸Šæµ· VPN å¦‚æœæ‚¨ä½äºä¸­å›½(å«æ¸¯ï¼Œæ¾³ï¼Œå°) éœ€è¦è°ƒç”¨ OpenAI æˆ–è€… å…¶ä»–å¢ƒå¤–æ¨¡å‹çš„ APIï¼Œéœ€è¦ä½¿ç”¨ VPN å·¥å…·æˆ–è®¿é—®é•œåƒç«™ã€‚ ä» Huggingface ä¸‹è½½æ¨¡å‹æˆ–è€…ä»æœ¬ä»“åº“æ‹‰å–æœ€æ–°çš„ä»£ç æ—¶ï¼Œéœ€è¦å¼€å‘è€…è‡ªè¡Œè®¾ç½®ä»£ç†ã€‚æœ¬é¡¹ç›®ä¸æ¶‰åŠä»»ä½•ä»£ç†å·¥å…·è®¾ç½®å’Œä½¿ç”¨ï¼Œä¹Ÿä¸è§£å†³ä»»ä½•å…³äºä»£ç†çš„é—®é¢˜ã€‚ Docker éƒ¨ç½² å¼€å‘ç»„ä¸ºå¼€å‘è€…ä»¬æä¾›äº†ä¸€é”®éƒ¨ç½²çš„ docker é•œåƒæ–‡ä»¶æ‡’äººåŒ…ã€‚å¼€å‘è€…ä»¬å¯ä»¥åœ¨ AutoDL å¹³å°å’Œ Docker å¹³å°ä¸€é”®éƒ¨ç½²ã€‚ ğŸŒ AutoDL é•œåƒ ï¼Œå·²ç»æ›´æ–°åˆ°V13ç‰ˆæœ¬,å¯¹åº”0.2.9 ğŸ³ Docker é•œåƒ ğŸ’» æœ¬æ¬¡æ›´æ–°ååŒæ—¶æ”¯æŒ DockerHubã€é˜¿é‡Œäº‘ã€è…¾è®¯äº‘é•œåƒæº ğŸŒ²ï¼š docker run -d --gpus all -p 80:8501 isafetech/chatchat:0.2.10 docker run -d --gpus all -p 80:8501 ccr.ccs.tencentyun.com/chatchat/chatchat:0.2.10 docker run -d --gpus all -p 80:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.10 è¯¥ç‰ˆæœ¬é•œåƒå¤§å° 50.1GBï¼Œä½¿ç”¨ v0.2.10ï¼Œä»¥ nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04 ä¸ºåŸºç¡€é•œåƒ è¯¥ç‰ˆæœ¬ä¸ºæ­£å¸¸ç‰ˆæœ¬ï¼Œéè½»é‡åŒ–ç‰ˆæœ¬ è¯¥ç‰ˆæœ¬å†…ç½®å¹¶é»˜è®¤å¯ç”¨ä¸€ä¸ª Embedding æ¨¡å‹ï¼šbge-large-zh-v1.5ï¼Œå†…ç½®å¹¶é»˜è®¤å¯ç”¨ ChatGLM3-6B è¯¥ç‰ˆæœ¬ç›®æ ‡ä¸ºæ–¹ä¾¿ä¸€é”®éƒ¨ç½²ä½¿ç”¨ï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»åœ¨ Linux å‘è¡Œç‰ˆä¸Šå®‰è£…äº† NVIDIA é©±åŠ¨ç¨‹åº è¯·æ³¨æ„ï¼Œæ‚¨ä¸éœ€è¦åœ¨ä¸»æœºç³»ç»Ÿä¸Šå®‰è£… CUDA å·¥å…·åŒ…ï¼Œä½†éœ€è¦å®‰è£… NVIDIA Driver ä»¥åŠ NVIDIA Container Toolkitï¼Œè¯·å‚è€ƒå®‰è£…æŒ‡å— é¦–æ¬¡æ‹‰å–å’Œå¯åŠ¨å‡éœ€è¦ä¸€å®šæ—¶é—´ï¼Œé¦–æ¬¡å¯åŠ¨æ—¶è¯·å‚ç…§ä¸‹å›¾ä½¿ç”¨ docker logs -f æŸ¥çœ‹æ—¥å¿— å¦‚é‡åˆ°å¯åŠ¨è¿‡ç¨‹å¡åœ¨ Waiting.. æ­¥éª¤ï¼Œå»ºè®®ä½¿ç”¨ docker exec -it bash è¿›å…¥ logs/ ç›®å½•æŸ¥çœ‹å¯¹åº”é˜¶æ®µæ—¥å¿— å¸¸è§„æ¨¡å¼æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆ # é¦–å…ˆï¼Œç¡®ä¿¡ä½ çš„æœºå™¨å®‰è£…äº† Python 3.8 - 3.10 ç‰ˆæœ¬ $ python --version Python 3.8.13 # å¦‚æœä½äºè¿™ä¸ªç‰ˆæœ¬ï¼Œå¯ä½¿ç”¨condaå®‰è£…ç¯å¢ƒ $ conda create -p /your_path/env_name python=3.8 # æ¿€æ´»ç¯å¢ƒ $ source activate /your_path/env_name # æˆ–ï¼Œcondaå®‰è£…ï¼Œä¸æŒ‡å®šè·¯å¾„, æ³¨æ„ä»¥ä¸‹ï¼Œéƒ½å°†/your_path/env_nameæ›¿æ¢ä¸ºenv_name $ conda create -n env_name python=3.8 $ conda activate env_name # Activate the environment # æ›´æ–°pyåº“ $ pip3 install --upgrade pip # å…³é—­ç¯å¢ƒ $ source deactivate /your_path/env_name # åˆ é™¤ç¯å¢ƒ $ conda env remove -p /your_path/env_name æ¥ç€ï¼Œå¼€å§‹å®‰è£…é¡¹ç›®çš„ä¾èµ– # æ‹‰å–ä»“åº“ $ git clone --recursive https://github.com/chatchat-space/Langchain-Chatchat.git # è¿›å…¥ç›®å½• $ cd Langchain-Chatchat # å®‰è£…å…¨éƒ¨ä¾èµ– $ pip install -r requirements.txt # é»˜è®¤ä¾èµ–åŒ…æ‹¬åŸºæœ¬è¿è¡Œç¯å¢ƒï¼ˆFAISSå‘é‡åº“ï¼‰ã€‚ä»¥ä¸‹æ˜¯å¯é€‰ä¾èµ–ï¼š - å¦‚æœè¦ä½¿ç”¨ milvus/pg_vector ç­‰å‘é‡åº“ï¼Œè¯·å°† requirements.txt ä¸­ç›¸åº”ä¾èµ–å–æ¶ˆæ³¨é‡Šå†å®‰è£…ã€‚ - å¦‚æœè¦å¼€å¯ OCR GPU åŠ é€Ÿï¼Œè¯·å®‰è£… rapidocr_paddle[gpu] - å¦‚æœè¦ä½¿ç”¨åœ¨çº¿ API æ¨¡å‹ï¼Œè¯·å®‰è£…å¯¹ç”¨çš„ SDK æ­¤å¤–ï¼Œä¸ºæ–¹ä¾¿ç”¨æˆ· API ä¸ webui åˆ†ç¦»è¿è¡Œï¼Œå¯å•ç‹¬æ ¹æ®è¿è¡Œéœ€æ±‚å®‰è£…ä¾èµ–åŒ…ã€‚ å¦‚æœåªéœ€è¿è¡Œ APIï¼Œå¯æ‰§è¡Œï¼š $ pip install -r requirements_api.txt # é»˜è®¤ä¾èµ–åŒ…æ‹¬åŸºæœ¬è¿è¡Œç¯å¢ƒï¼ˆFAISSå‘é‡åº“ï¼‰ã€‚å¦‚æœè¦ä½¿ç”¨ milvus/pg_vector ç­‰å‘é‡åº“ï¼Œè¯·å°† requirements.txt ä¸­ç›¸åº”ä¾èµ–å–æ¶ˆæ³¨é‡Šå†å®‰è£…ã€‚ å¦‚æœåªéœ€è¿è¡Œ WebUIï¼Œå¯æ‰§è¡Œï¼š $ pip install -r requirements_webui.txt æ³¨ï¼šä½¿ç”¨ langchain.document_loaders.UnstructuredFileLoaderè¿›è¡Œ .docx ç­‰æ ¼å¼éç»“æ„åŒ–æ–‡ä»¶æ¥å…¥æ—¶ï¼Œå¯èƒ½éœ€è¦ä¾æ®æ–‡æ¡£è¿›è¡Œå…¶ä»–ä¾èµ–åŒ…çš„å®‰è£…ï¼Œè¯·å‚è€ƒ langchain æ–‡æ¡£ã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºä»¥ä¸‹ä¾èµ–ï¼Œæˆ‘ä»¬å»ºè®®æºç å®‰è£…ä¾èµ–æˆ–è€…å®šæœŸæ£€æŸ¥æ˜¯å¦ä¸ºæœ€æ–°ç‰ˆæœ¬ï¼Œæˆ‘ä»¬çš„æ¡†æ¶å¯èƒ½ä¼šå¤§é‡ä½¿ç”¨è¿™äº›ä¾èµ–çš„æœ€æ–°ç‰¹æ€§ã€‚ transformers fastchat fastapi streamlit ä»¥åŠå…¶ç»„ä»¶ langchain ä»¥åŠå…¶ç»„ä»¶ xformers æ¨¡å‹ä¸‹è½½ å¦‚éœ€åœ¨æœ¬åœ°æˆ–ç¦»çº¿ç¯å¢ƒä¸‹è¿è¡Œæœ¬é¡¹ç›®ï¼Œéœ€è¦é¦–å…ˆå°†é¡¹ç›®æ‰€éœ€çš„æ¨¡å‹ä¸‹è½½è‡³æœ¬åœ°ï¼Œé€šå¸¸å¼€æº LLM ä¸ Embedding æ¨¡å‹å¯ä»¥ä» HuggingFace ä¸‹è½½ã€‚ ä»¥æœ¬é¡¹ç›®ä¸­é»˜è®¤ä½¿ç”¨çš„ LLM æ¨¡å‹ THUDM/chatglm3-6b ä¸ Embedding æ¨¡å‹ BAAI/bge-large-zh-v1.5 ä¸ºä¾‹ï¼š ä¸‹è½½æ¨¡å‹éœ€è¦å…ˆå®‰è£…Git LFSï¼Œç„¶åè¿è¡Œ $ git lfs install $ git clone https://huggingface.co/THUDM/chatglm2-6b $ git clone https://huggingface.co/moka-ai/m3e-base åˆå§‹åŒ–çŸ¥è¯†åº“ å½“å‰é¡¹ç›®çš„çŸ¥è¯†åº“ä¿¡æ¯å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼Œåœ¨æ­£å¼è¿è¡Œé¡¹ç›®ä¹‹å‰è¯·å…ˆåˆå§‹åŒ–æ•°æ®åº“ï¼ˆæˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨åœ¨æ‰§è¡Œæ“ä½œå‰å¤‡ä»½æ‚¨çš„çŸ¥è¯†æ–‡ä»¶ï¼‰ã€‚ å¦‚æœæ‚¨å·²ç»æœ‰åˆ›å»ºè¿‡çŸ¥è¯†åº“ï¼Œå¯ä»¥å…ˆæ‰§è¡Œä»¥ä¸‹å‘½ä»¤åˆ›å»ºæˆ–æ›´æ–°æ•°æ®åº“è¡¨ï¼š $ python init_database.py --create-tables å¦‚æœå¯ä»¥æ­£å¸¸è¿è¡Œï¼Œåˆ™æ— éœ€å†é‡å»ºçŸ¥è¯†åº“ã€‚ å¦‚æœæ‚¨æ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œæœ¬é¡¹ç›®ï¼ŒçŸ¥è¯†åº“å°šæœªå»ºç«‹ï¼Œæˆ–è€…ä¹‹å‰ä½¿ç”¨çš„æ˜¯ä½äºæœ€æ–°masteråˆ†æ”¯ç‰ˆæœ¬çš„æ¡†æ¶ï¼Œæˆ–è€…é…ç½®æ–‡ä»¶ä¸­çš„çŸ¥è¯†åº“ç±»å‹ã€åµŒå…¥æ¨¡å‹å‘ç”Ÿå˜åŒ–ï¼Œæˆ–è€…ä¹‹å‰çš„å‘é‡åº“æ²¡æœ‰å¼€å¯ normalize_L2ï¼Œéœ€è¦ä»¥ä¸‹å‘½ä»¤åˆå§‹åŒ–æˆ–é‡å»ºçŸ¥è¯†åº“ï¼š $ python init_database.py --recreate-vs ä¸€é”®å¯åŠ¨ å¯åŠ¨å‰ï¼Œç¡®ä¿å·²ç»æŒ‰ç…§å‚æ•°é…ç½®æ­£ç¡®é…ç½®å„configæ¨¡å—ã€‚ ä¸€é”®å¯åŠ¨è„šæœ¬ startup.pyï¼Œ ä¸€é”®å¯åŠ¨æ‰€æœ‰ Fastchat æœåŠ¡ã€API æœåŠ¡ã€WebUI æœåŠ¡ï¼Œç¤ºä¾‹ä»£ç ï¼š $ python startup.py -a å¹¶å¯ä½¿ç”¨ Ctrl + C ç›´æ¥å…³é—­æ‰€æœ‰è¿è¡ŒæœåŠ¡ã€‚å¦‚æœä¸€æ¬¡ç»“æŸä¸äº†ï¼Œå¯ä»¥å¤šæŒ‰å‡ æ¬¡ã€‚ å¯é€‰å‚æ•°åŒ…æ‹¬ -a (æˆ–--all-webui), --all-api, --llm-api, -c (æˆ–--controller), --openai-api, -m (æˆ–--model-worker), --api, --webuiï¼Œå…¶ä¸­ï¼š --all-webui ä¸ºä¸€é”®å¯åŠ¨ WebUI æ‰€æœ‰ä¾èµ–æœåŠ¡ï¼› --all-api ä¸ºä¸€é”®å¯åŠ¨ API æ‰€æœ‰ä¾èµ–æœåŠ¡ï¼› --llm-api ä¸ºä¸€é”®å¯åŠ¨ Fastchat æ‰€æœ‰ä¾èµ–çš„ LLM æœåŠ¡ï¼› --openai-api ä¸ºä»…å¯åŠ¨ FastChat çš„ controller å’Œ openai-api-server æœåŠ¡ï¼› å…¶ä»–ä¸ºå•ç‹¬æœåŠ¡å¯åŠ¨é€‰é¡¹ã€‚ è‹¥æƒ³æŒ‡å®šéé»˜è®¤æ¨¡å‹ï¼Œéœ€è¦ç”¨ --model-name é€‰é¡¹ï¼Œç¤ºä¾‹ï¼š $ python startup.py --all-webui --model-name Qwen-7B-Chat æ›´å¤šä¿¡æ¯å¯é€šè¿‡ python startup.py -h æŸ¥çœ‹ã€‚ å¤šå¡åŠ è½½ é¡¹ç›®æ”¯æŒå¤šå¡åŠ è½½ï¼Œéœ€åœ¨ startup.py ä¸­çš„ create_model_worker_app å‡½æ•°ä¸­ï¼Œä¿®æ”¹å¦‚ä¸‹ä¸‰ä¸ªå‚æ•°: gpus=None, num_gpus= 1, max_gpu_memory=\"20GiB\" å…¶ä¸­ï¼Œgpus æ§åˆ¶ä½¿ç”¨çš„æ˜¾å¡çš„IDï¼Œä¾‹å¦‚ \"0,1\"; num_gpus æ§åˆ¶ä½¿ç”¨çš„å¡æ•°; max_gpu_memory æ§åˆ¶æ¯ä¸ªå¡ä½¿ç”¨çš„æ˜¾å­˜å®¹é‡ã€‚ æ³¨1ï¼šserver_config.pyçš„FSCHAT_MODEL_WORKERSå­—å…¸ä¸­ä¹Ÿå¢åŠ äº†ç›¸å…³é…ç½®ï¼Œå¦‚æœ‰éœ€è¦ä¹Ÿå¯é€šè¿‡ä¿®æ”¹FSCHAT_MODEL_WORKERSå­—å…¸ä¸­å¯¹åº”å‚æ•°å®ç°å¤šå¡åŠ è½½ï¼Œä¸”éœ€æ³¨æ„server_config.pyçš„é…ç½®ä¼šè¦†ç›–create_model_worker_app å‡½æ•°çš„é…ç½®ã€‚ æ³¨2ï¼šå°‘æ•°æƒ…å†µä¸‹ï¼Œgpuså‚æ•°ä¼šä¸ç”Ÿæ•ˆï¼Œæ­¤æ—¶éœ€è¦é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡CUDA_VISIBLE_DEVICESæ¥æŒ‡å®štorchå¯è§çš„gpu,ç¤ºä¾‹ä»£ç ï¼š CUDA_VISIBLE_DEVICES=0,1 python startup.py -a æœ€è½»æ¨¡å¼æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆ è¯¥æ¨¡å¼çš„é…ç½®æ–¹å¼ä¸å¸¸è§„æ¨¡å¼ç›¸åŒï¼Œä½†æ— éœ€å®‰è£… torch ç­‰é‡ä¾èµ–ï¼Œé€šè¿‡åœ¨çº¿APIå®ç° LLM å’Œ Ebeddings ç›¸å…³åŠŸèƒ½ï¼Œé€‚åˆæ²¡æœ‰æ˜¾å¡çš„ç”µè„‘ä½¿ç”¨ã€‚ $ pip install -r requirements_lite.txt $ python startup.py -a --lite è¯¥æ¨¡å¼æ”¯æŒçš„åœ¨çº¿ Embeddings åŒ…æ‹¬ï¼š æ™ºè°±AI MiniMax ç™¾åº¦åƒå¸† é˜¿é‡Œäº‘é€šä¹‰åƒé—® åœ¨ model_config.py ä¸­ å°† LLM_MODELS å’Œ EMBEDDING_MODEL è®¾ç½®ä¸ºå¯ç”¨çš„åœ¨çº¿ API åç§°å³å¯ã€‚ æ³¨æ„ï¼šåœ¨å¯¹è¯è¿‡ç¨‹ä¸­å¹¶ä¸è¦æ±‚ LLM æ¨¡å‹ä¸ Embeddings æ¨¡å‹ä¸€è‡´ï¼Œä½ å¯ä»¥åœ¨çŸ¥è¯†åº“ç®¡ç†é¡µé¢ä¸­ä½¿ç”¨ zhipu-api ä½œä¸ºåµŒå…¥æ¨¡å‹ï¼Œåœ¨çŸ¥è¯†åº“å¯¹è¯é¡µé¢ä½¿ç”¨å…¶å®ƒæ¨¡å‹ã€‚ Â© snowdreams1006 all right reservedï¼Œpowered by Gitbookæ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š 2024-05-17 23:23:32 "},"LLM/å‚æ•°é…ç½®.html":{"url":"LLM/å‚æ•°é…ç½®.html","title":"å‚æ•°é…ç½®","keywords":"","body":"åœ¨å¼€å§‹å‚æ•°é…ç½®ä¹‹å‰ï¼Œå…ˆæ‰§è¡Œä»¥ä¸‹è„šæœ¬ python copy_config_example.py è¯¥è„šæœ¬å°†ä¼šå°†æ‰€æœ‰configç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶æ ·ä¾‹å¤åˆ¶ä¸€ä»½åˆ°configç›®å½•ä¸‹ï¼Œæ–¹ä¾¿å¼€å‘è€…è¿›è¡Œé…ç½®ã€‚ æ¥ç€ï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚ï¼Œå¯¹é…ç½®æ–‡ä»¶è¿›è¡Œä¿®æ”¹ã€‚ åŸºç¡€é…ç½®é¡¹ basic_config.py è¯¥é…ç½®åŸºè´Ÿè´£è®°å½•æ—¥å¿—çš„æ ¼å¼å’Œå‚¨å­˜è·¯å¾„ï¼Œé€šå¸¸ä¸éœ€è¦ä¿®æ”¹ã€‚ æ¨¡å‹é…ç½®é¡¹ model_config.py æœ¬æ–‡ä»¶åŒ…å«æœ¬åœ°LLMæ¨¡å‹ã€æœ¬åœ°Embeddingsæ¨¡å‹ã€åœ¨çº¿LLMæ¨¡å‹APIçš„ç›¸å…³é…ç½®ã€‚ æœ¬åœ°æ¨¡å‹è·¯å¾„é…ç½®ã€‚å»ºè®®å°†æ‰€æœ‰ä¸‹è½½çš„æ¨¡å‹æ”¾åˆ°ä¸€ä¸ªç»Ÿä¸€çš„ç›®å½•ä¸‹ï¼Œç„¶åå°†MODEL_ROOT_PATHæŒ‡å®šä¸ºè¯¥ç›®å½•ï¼Œåªè¦æ¨¡å‹ç›®å½•åç§°ç¬¦åˆä¸‹åˆ—æƒ…å†µä¹‹ä¸€ï¼ˆä»¥text2vecä¸ºä¾‹ï¼‰ï¼Œå³å¯è‡ªåŠ¨è¯†åˆ«åŠ è½½ï¼š text2vecï¼Œå³MODEL_PATHä¸­çš„é”® GanymedeNil/text2vec-large-chineseï¼Œå³MODEL_PATHä¸­çš„å€¼ text2vec-large-chineseï¼Œå³MODEL_PATHä¸­çš„å€¼çš„ç®€å†™å½¢å¼ åœ¨çº¿æ¨¡å‹APIé…ç½®ã€‚åœ¨ONLINE_LLM_MODELå·²ç»é¢„å…ˆå†™å¥½äº†æ‰€æœ‰æ”¯æŒçš„åœ¨çº¿APIæœåŠ¡ï¼Œé€šå¸¸åªéœ€è¦æŠŠç”³è¯·çš„API_KEYç­‰å¡«å…¥å³å¯ã€‚ æœ‰äº›åœ¨çº¿APIæœåŠ¡éœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ–ï¼š zhipu-api: zhipuai fangzhou-api: volcengine>=1.0.106 qianfan-api: qianfan qwen-api: dashscope HISTORY_LENã€‚å†å²å¯¹è¯è½®æ•°é€šå¸¸ä¸å»ºè®®è®¾ç½®è¶…è¿‡10ï¼Œå› ä¸ºè¿™å¯èƒ½å¯¼è‡´ä»¥ä¸‹é—®é¢˜ æ˜¾å­˜å ç”¨è¿‡é«˜ï¼šå°¤å…¶æ˜¯éƒ¨åˆ†æ¨¡å‹ï¼Œæœ¬èº«å°±å·²ç»è¦å ç”¨æ»¡æ˜¾å­˜çš„æƒ…å†µä¸‹ï¼Œä¿ç•™å¤ªå¤šå†å²ï¼Œä¸€æ¬¡ä¼ å…¥tokenå¤ªå¤šï¼Œå¯èƒ½ä¼šçˆ†æ˜¾å­˜ã€‚ é€Ÿåº¦å¤„ç†å¾ˆæ…¢ï¼šè¿˜æ˜¯å› ä¸ºä¸€æ¬¡ä¼ å…¥äº†å¤ªå¤štokenï¼Œå¯¼è‡´é€Ÿåº¦å¾ˆæ…¢ã€‚ TEMPERATUREã€‚é€šå¸¸ä¸å»ºè®®è®¾ç½®è¿‡é«˜ã€‚ åœ¨Agentå¯¹è¯æ¨¡å¼å’ŒçŸ¥è¯†åº“é—®ç­”ä¸­ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®å°†è¦å…¶è®¾ç½®æˆ0æˆ–è€…æ¥è¿‘äº0ã€‚ Agent_MODEL = None æˆ‘ä»¬æ”¯æŒç”¨æˆ·ä½¿ç”¨â€œæ¨¡å‹æ¥åŠ›èµ›â€çš„ç”¨æ³•ï¼Œå³ï¼š é€‰æ‹©çš„å¤§æ¨¡å‹ä»…èƒ½è°ƒç”¨å·¥å…·ï¼Œä½†æ˜¯åœ¨å·¥å…·ä¸­è¡¨ç°è¾ƒå·®ï¼Œåˆ™è¿™ä¸ªå·¥å…·ä½œä¸º â€œæ¨¡å‹è°ƒç”¨å·¥å…·â€ å¦‚æœç”¨æˆ·è®¾ç½®äº†Agent_MODELï¼Œåˆ™åœ¨ Agent ä¸­ï¼Œä½¿ç”¨Agent_MODELæ¥æ‰§è¡Œä»»åŠ¡ï¼Œå¦åˆ™ï¼Œä½¿ç”¨LLM_MODEL æç¤ºè¯é…ç½®é¡¹ prompt_config.py æç¤ºè¯é…ç½®åˆ†ä¸ºä¸‰ä¸ªæ¿å—ï¼Œåˆ†åˆ«å¯¹åº”ä¸‰ç§èŠå¤©ç±»å‹ã€‚ llm_chat: åŸºç¡€çš„å¯¹è¯æç¤ºè¯ï¼Œ é€šå¸¸æ¥è¯´ï¼Œç›´æ¥æ˜¯ç”¨æˆ·è¾“å…¥çš„å†…å®¹ï¼Œæ²¡æœ‰ç³»ç»Ÿæç¤ºè¯ã€‚ knowledge_base_chat: ä¸çŸ¥è¯†åº“å¯¹è¯çš„æç¤ºè¯ï¼Œåœ¨æ¨¡æ¿ä¸­ï¼Œæˆ‘ä»¬ä¸ºå¼€å‘è€…è®¾è®¡äº†ä¸€ä¸ªç³»ç»Ÿæç¤ºè¯ï¼Œå¼€å‘è€…å¯ä»¥è‡ªè¡Œæ›´æ”¹ã€‚ agent_chat: ä¸Agentå¯¹è¯çš„æç¤ºè¯ï¼ŒåŒæ ·ï¼Œæˆ‘ä»¬ä¸ºå¼€å‘è€…è®¾è®¡äº†ä¸€ä¸ªç³»ç»Ÿæç¤ºè¯ï¼Œå¼€å‘è€…å¯ä»¥è‡ªè¡Œæ›´æ”¹ã€‚ promptæ¨¡æ¿ä½¿ç”¨Jinja2è¯­æ³•ï¼Œç®€å•ç‚¹å°±æ˜¯ç”¨åŒå¤§æ‹¬å·ä»£æ›¿f-stringçš„å•å¤§æ‹¬å· è¯·æ³¨æ„ï¼Œæœ¬é…ç½®æ–‡ä»¶æ”¯æŒçƒ­åŠ è½½ï¼Œä¿®æ”¹promptæ¨¡æ¿åæ— éœ€é‡å¯æœåŠ¡ã€‚ æ•°æ®åº“é…ç½® kb_config.py è¯·ç¡®è®¤æœ¬åœ°åˆ†è¯å™¨è·¯å¾„æ˜¯å¦å·²ç»å¡«å†™ï¼Œå¦‚ï¼š text_splitter_dict = { \"ChineseRecursiveTextSplitter\": { \"source\":\"huggingface\", # é€‰æ‹©tiktokenåˆ™ä½¿ç”¨openaiçš„æ–¹æ³•,ä¸å¡«å†™åˆ™é»˜è®¤ä¸ºå­—ç¬¦é•¿åº¦åˆ‡å‰²æ–¹æ³•ã€‚ \"tokenizer_name_or_path\":\"\", # ç©ºæ ¼ä¸å¡«åˆ™é»˜è®¤ä½¿ç”¨å¤§æ¨¡å‹çš„åˆ†è¯å™¨ã€‚ } } è®¾ç½®å¥½çš„åˆ†è¯å™¨éœ€è¦å†TEXT_SPLITTER_NAMEä¸­æŒ‡å®šå¹¶åº”ç”¨ã€‚ åœ¨è¿™é‡Œï¼Œé€šå¸¸ä½¿ç”¨huggingfaceçš„æ–¹æ³•ï¼Œå¹¶ä¸”ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨å¤§æ¨¡å‹è‡ªå¸¦çš„åˆ†è¯å™¨æ¥å®Œæˆä»»åŠ¡ã€‚ è¯·æ³¨æ„ï¼Œä½¿ç”¨gpt2åˆ†è¯å™¨å°†è¦è®¿é—®huggingfaceå®˜ç½‘ä¸‹è½½æƒé‡ã€‚ æˆ‘ä»¬è¿˜æ”¯æŒä½¿ç”¨tiktoken å’Œä¼ ç»Ÿçš„ æŒ‰ç…§é•¿åº¦åˆ†è¯çš„æ–¹å¼ï¼Œå¼€å‘è€…å¯ä»¥è‡ªè¡Œé…ç½®ã€‚ å¦‚æœå¸Œæœ›è°ƒç”¨è‡ªå·±çš„åˆ†è¯å™¨ï¼Œè¯·å‚è€ƒ[æœ€ä½³å®è·µ]éƒ¨åˆ†ã€‚ - ```faiss```: ä½¿ç”¨faissæ•°æ®åº“ï¼Œéœ€è¦å®‰è£…faiss-gpu - ```milvus```: ä½¿ç”¨milvusæ•°æ®åº“ï¼Œéœ€è¦å®‰è£…milvuså¹¶è¿›è¡Œç«¯å£é…ç½® - ```pg```: ä½¿ç”¨pgæ•°æ®åº“ï¼Œéœ€è¦é…ç½®connection_uri ## æœåŠ¡å’Œç«¯å£é…ç½®é¡¹ server_config.py é€šå¸¸ï¼Œè¿™ä¸ªé¡µé¢å¹¶ä¸éœ€è¦è¿›è¡Œå¤§é‡çš„ä¿®æ”¹ï¼Œä»…éœ€ç¡®ä¿å¯¹åº”çš„ç«¯å£æ‰“å¼€ï¼Œå¹¶ä¸äº’ç›¸å†²çªå³å¯ã€‚ å¦‚æœä½ æ˜¯Linuxç³»ç»Ÿæ¨èè®¾ç½® DEFAULT_BIND_HOST =\"0.0.0.0\" å¦‚æœä½¿ç”¨è”ç½‘æ¨¡å‹ï¼Œåˆ™éœ€è¦å…³æ³¨è”ç½‘æ¨¡å‹çš„ç«¯å£ã€‚ è¿™äº›æ¨¡å‹å¿…é¡»æ˜¯åœ¨model_config.MODEL_PATHæˆ–ONLINE_MODELä¸­æ­£ç¡®é…ç½®çš„ã€‚ #åœ¨å¯åŠ¨startup.pyæ—¶ï¼Œå¯ç”¨é€šè¿‡`--model-worker --model-name xxxx`æŒ‡å®šæ¨¡å‹ï¼Œä¸æŒ‡å®šåˆ™ä¸ºLLM_MODEL ## è¦†ç›–é…ç½®æ–‡ä»¶ æˆ–è€…é…ç½® startup.py åœ¨ ```server_config.py```ä¸­æœ‰ä»¥ä¸‹é…ç½®æ–‡ä»¶è¢«æ³¨é‡Šäº† \"gpus\": None, # ä½¿ç”¨çš„GPUï¼Œä»¥strçš„æ ¼å¼æŒ‡å®šï¼Œå¦‚\"0,1\"ï¼Œå¦‚å¤±æ•ˆè¯·ä½¿ç”¨CUDA_VISIBLE_DEVICES=\"0,1\"ç­‰å½¢å¼æŒ‡å®š \"num_gpus\": 1, # ä½¿ç”¨GPUçš„æ•°é‡ \"max_gpu_memory\":\"20GiB\", # æ¯ä¸ªGPUå ç”¨çš„æœ€å¤§æ˜¾å­˜ ä»¥ä¸‹ä¸ºmodel_workeréå¸¸ç”¨å‚æ•°ï¼Œå¯æ ¹æ®éœ€è¦é…ç½® \"load_8bit\": False, # å¼€å¯8bité‡åŒ– \"cpu_offloading\": None, \"gptq_ckpt\": None, \"gptq_wbits\": 16, \"gptq_groupsize\": -1, \"gptq_act_order\": False, \"awq_ckpt\": None, \"awq_wbits\": 16, \"awq_groupsize\": -1, \"model_names\": [LLM_MODEL], \"conv_template\": None, \"limit_worker_concurrency\": 5, \"stream_interval\": 2, \"no_register\": False, \"embed_in_truncate\": False, ä»¥ä¸‹ä¸ºvllm_wokeré…ç½®å‚æ•°,æ³¨æ„ä½¿ç”¨vllmå¿…é¡»æœ‰gpuï¼Œä»…åœ¨Linuxæµ‹è¯•é€šè¿‡ tokenizer = model_path # å¦‚æœtokenizerä¸model_pathä¸ä¸€è‡´åœ¨æ­¤å¤„æ·»åŠ  'tokenizer_mode':'auto', 'trust_remote_code':True, 'download_dir':None, 'load_format':'auto', 'dtype':'auto', 'seed':0, 'worker_use_ray':False, 'pipeline_parallel_size':1, 'tensor_parallel_size':1, 'block_size':16, 'swap_space':4 , # GiB 'gpu_memory_utilization':0.90, 'max_num_batched_tokens':2560, 'max_num_seqs':256, 'disable_log_stats':False, 'conv_template':None, 'limit_worker_concurrency':5, 'no_register':False, 'num_gpus': 1 'engine_use_ray': False, 'disable_log_requests': False åœ¨è¿™äº›å‚æ•°ä¸­ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œåˆ™ä½¿ç”¨```startup.py```ä¸­çš„é»˜è®¤å€¼ï¼Œå¦‚æœè®¾ç½®äº†ï¼Œåˆ™ä½¿ç”¨è®¾ç½®çš„å€¼ã€‚ å› æ­¤ï¼Œå¼ºçƒˆå»ºè®®å¼€å‘ä¸è¦åœ¨```startup.py```ä¸­è¿›è¡Œé…ç½®ï¼Œè€Œåº”è¯¥åœ¨```server_config.py```ä¸­è¿›è¡Œé…ç½®ã€‚é¿å…é…ç½®æ–‡ä»¶è¦†ç›–ã€‚ ## é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹ åœ¨```model_config.py```å®Œæˆæ¨¡å‹é…ç½®åï¼Œè¿˜ä¸èƒ½ç›´æ¥ä½¿ç”¨ï¼Œéœ€è¦åœ¨è¯¥æ–‡ä»¶ä¸‹é…ç½®æœ¬åœ°æ¨¡å‹çš„è¿è¡Œæ–¹å¼æˆ–åœ¨çº¿æ¨¡å‹çš„APIï¼Œä¾‹å¦‚ \"agentlm-7b\": { # ä½¿ç”¨defaultä¸­çš„IPå’Œç«¯å£ \"device\": \"cuda\", }, \"zhipu-api\": { # è¯·ä¸ºæ¯ä¸ªè¦è¿è¡Œçš„åœ¨çº¿APIè®¾ç½®ä¸åŒçš„ç«¯å£ \"port\": 21001, }, ``` æœ¬åœ°æ¨¡å‹ä½¿ç”¨defaultä¸­çš„IPå’Œç«¯å£ï¼Œåœ¨çº¿æ¨¡å‹å¯ä»¥è‡ªå·±é€‰æ‹©ç«¯å£ Â© snowdreams1006 all right reservedï¼Œpowered by Gitbookæ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š 2024-05-17 23:23:32 "},"LLM/è‡ªå®šä¹‰.html":{"url":"LLM/è‡ªå®šä¹‰.html","title":"è‡ªå®šä¹‰","keywords":"","body":"ä½¿ç”¨è‡ªå®šä¹‰çš„åˆ†è¯å™¨ åœ¨text_splitteræ–‡ä»¶å¤¹ä¸‹æ–°å»ºä¸€ä¸ªæ–‡ä»¶ï¼Œæ–‡ä»¶åä¸ºæ‚¨çš„åˆ†è¯å™¨åå­—ï¼Œæ¯”å¦‚my_splitter.pyï¼Œç„¶ååœ¨__init__.pyä¸­å¯¼å…¥æ‚¨çš„åˆ†è¯å™¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š from .my_splitter import MySplitter ä¿®æ”¹config/kb_config.pyæ–‡ä»¶ï¼Œå°†æ‚¨çš„åˆ†è¯å™¨åå­—æ·»åŠ åˆ°text_splitter_dictä¸­ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š MySplitter: { \"source\": \"huggingface\", # é€‰æ‹©tiktokenåˆ™ä½¿ç”¨openaiçš„æ–¹æ³• \"tokenizer_name_or_path\": \"your tokenizer\", #å¦‚æœé€‰æ‹©huggingfaceåˆ™ä½¿ç”¨huggingfaceçš„æ–¹æ³•ï¼Œéƒ¨åˆ†tokenizeréœ€è¦ä»Huggingfaceä¸‹è½½ } TEXT_SPLITTER = \"MySplitter\" å®Œæˆä¸Šè¿°æ­¥éª¤åï¼Œå°±èƒ½ä½¿ç”¨è‡ªå·±çš„åˆ†è¯å™¨äº†ã€‚ ä½¿ç”¨è‡ªå®šä¹‰çš„ Agent å·¥å…· åˆ›å»ºè‡ªå·±çš„Agentå·¥å…· å¼€å‘è€…åœ¨server/agentæ–‡ä»¶ä¸­åˆ›å»ºä¸€ä¸ªè‡ªå·±çš„æ–‡ä»¶ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°tools_select.pyä¸­ã€‚è¿™æ ·å°±å®Œæˆäº†Toolsçš„è®¾å®šã€‚ å½“æ‚¨åˆ›å»ºäº†ä¸€ä¸ªcustom_agent.pyæ–‡ä»¶ï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªworkå‡½æ•°ï¼Œé‚£ä¹ˆæ‚¨éœ€è¦åœ¨tools_select.pyä¸­æ·»åŠ å¦‚ä¸‹ä»£ç ï¼š from custom_agent import work Tool.from_function( func=work, name=\"è¯¥å‡½æ•°çš„åå­—\", description=\"\" ) è¯·æ³¨æ„ï¼Œå¦‚æœä½ ç¡®å®šåœ¨æŸä¸€ä¸ªå·¥ç¨‹ä¸­ä¸ä¼šä½¿ç”¨åˆ°æŸä¸ªå·¥å…·ï¼Œå¯ä»¥å°†å…¶ä»Toolsä¸­ç§»é™¤ï¼Œé™ä½æ¨¡å‹åˆ†ç±»é”™è¯¯å¯¼è‡´ä½¿ç”¨é”™è¯¯å·¥å…·çš„é£é™©ã€‚ ä¿®æ”¹ custom_template.py æ–‡ä»¶ å¼€å‘è€…éœ€è¦æ ¹æ®è‡ªå·±é€‰æ‹©çš„å¤§æ¨¡å‹è®¾å®šé€‚åˆè¯¥æ¨¡å‹çš„Agent Promptå’Œè‡ªè‡ªå®šä¹‰è¿”å›æ ¼å¼ã€‚ \"\"\" Answer the following questions as best you can. You have access to the following tools: {tools} Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [{tool_names}] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can be repeated zero or more times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! history: {history} Question: {input} Thought: {agent_scratchpad} \"\"\" é™¤äº†ä½¿ç”¨ Zero React çš„æç¤ºè¯æ–¹æ¡ˆï¼Œå¼€å‘è€…å¯ä»¥è‡ªè¡Œå¯¹æç¤ºè¯è¿›è¡Œä¿®æ”¹ï¼Œæˆ–è€…ä½¿ç”¨ Langchain æä¾›çš„å…¶ä»–çš„Agentç»“æ„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨ä½¿ç”¨çš„æ¨¡å‹ä¸ºChatGLM3-6Bæ¨¡å‹ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¯ä»¥æ­£å¸¸è¿è¡ŒChatGLM3-6Bçš„Agentæç¤ºè¯ï¼Œè¯¥æç¤ºè¯ä¸ Langchain çš„ struct Agentç›¸ä¼¼ï¼Œå…¶å†…å®¹å¦‚ä¸‹ï¼š \"ChatGLM3\": \"\"\" You can answer using the tools, or answer directly using your knowledge without using the tools.Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools} Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input). Valid \"action\" values: \"Final Answer\" or [{tool_names}] Provide only ONE action per $JSON_BLOB, as shown: ``` \"action\": $TOOL_NAME, \"action_input\": $INPUT ``` Follow this format: Question: input question to answer Thought: consider previous and subsequent steps Action: ``` $JSON_BLOB ``` Observation: action result ... (repeat Thought/Action/Observation N times) Thought: I know what to respond Action: ``` \"action\": \"Final Answer\", \"action_input\": \"Final response to human\" Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:. history: {history} Question: {input} Thought: {agent_scratchpad} \"\"\", è®©ä¸æ”¯æŒ Langchain è°ƒç”¨æ–¹å¼çš„ä½†å…·å¤‡ Agent èƒ½åŠ›çš„æ¨¡å‹å±•ç°èƒ½åŠ› ä»¥ChatGLM3-6Bä¸ºä»£è¡¨çš„æ¨¡å‹ï¼Œè™½ç„¶å…·æœ‰ Function Call èƒ½åŠ›ï¼Œä½†å…¶å¯¹é½æ ¼å¼ä¸ Langchain æä¾›é»˜è®¤Agentæ ¼å¼å¹¶ä¸ç¬¦åˆï¼Œå› æ­¤æ— æ³•ä½¿ç”¨ Langchain è‡ªèº«èƒ½åŠ›å®ç° Function Callã€‚åœ¨æˆ‘ä»¬çš„æ¡†æ¶ä¸­ï¼Œæ‚¨å¯ä»¥åœ¨ server/Agent/custom_agent/ æ–‡ä»¶å¤¹ä¸­è‡ªè¡Œå¤ç°æ›´å¤šæ¨¡å‹çš„ Agent èƒ½åŠ›å®ç°ã€‚ åœ¨å®Œæˆä¸Šè¿°æ­¥éª¤ä¹‹åï¼Œæ‚¨è¿˜éœ€è¦åˆ°server/chat/agent_chat/ä¸­å¯¼å…¥æ‚¨çš„æ¨¡å—æ¥å®ç°ç‰¹æ®Šåˆ¤å®šã€‚ åŒæ—¶ï¼Œä½ åº”è¯¥åœ¨è°ƒç”¨å·¥å…·çš„æ—¶å€™ä½¿ç”¨è‡ªå®šä¹‰çš„æ¨¡æ¿ï¼Œæˆ‘ä»¬ä»¥GLMç³»åˆ—æ¨¡å‹è¿›è¡Œæ¼”ç¤ºï¼Œå¦‚æœæ‚¨åœ¨ä½¿ç”¨GLMæ¨¡å‹è¿›è¡Œå·¥å…·è°ƒç”¨ï¼Œä½ åº”è¯¥ä½¿ç”¨model_config.pyä¸­çš„ChatGLM3æ¨¡æ¿ã€‚ å±€é™æ€§ ç”±äº React Agent çš„è„†å¼±æ€§ï¼Œtemperature å‚æ•°çš„è®¾ç½®å¯¹äºæ¨¡å‹çš„æ•ˆæœæœ‰å¾ˆå¤§çš„å½±å“ã€‚æˆ‘ä»¬å»ºè®®å¼€å‘è€…åœ¨ä½¿ç”¨è‡ªå®šä¹‰ Agent æ—¶ï¼Œå¯¹äºä¸åŒçš„æ¨¡å‹ï¼Œå°†å…¶è®¾ç½®æˆ0.1ä»¥ä¸‹ï¼Œä»¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚ ç›®å‰ï¼Œå®˜æ–¹ä»…å¯¹ ChatGLM3-6B ä¸€ç§æ¨¡å‹è¿›è¡Œäº† é Langchain å¯¹é½æ ¼å¼ä¸‹çš„èƒ½åŠ›æ¿€æ´»ï¼Œæˆ‘ä»¬æ¬¢è¿å¼€å‘è€…è‡ªè¡Œæ¢ç´¢å…¶ä»–æ¨¡å‹ï¼Œå¹¶æäº¤å¯¹åº”çš„ PRï¼Œè®©æ¡†æ¶æ”¯æŒæ›´å¤šçš„ Agent æ¨¡å‹ã€‚ åœ¨0.2.xç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰å¯¹Planè¿›è¡Œä¼˜åŒ–ï¼Œå› æ­¤ï¼Œè¿ç»­è°ƒç”¨å·¥å…·çš„èƒ½åŠ›è¾ƒå·®ï¼Œæˆ‘ä»¬ä¼šåœ¨0.3.xä¸­ä¼˜åŒ–è¿™ä¸€é—®é¢˜ã€‚æ­¤å¤–ï¼Œç»è¿‡æµ‹è¯•ï¼Œæœ¬åœ°æ¨¡å‹åœ¨å·¥å…·è°ƒç”¨ä¸Šçš„è¡¨ç°ä¸å¦‚åœ¨çº¿æ¨¡å‹ï¼Œæˆ‘ä»¬æ›´æ¨èä½¿ç”¨ gpt4-1106-Preview æ¥å®Œæˆå·¥å…·è°ƒç”¨çš„ä»»åŠ¡ã€‚ ä½¿ç”¨è‡ªå®šä¹‰çš„å¾®è°ƒæ¨¡å‹ æœ¬é¡¹ç›®åŸºäº FastChat åŠ è½½ LLM æœåŠ¡ï¼Œæ•…éœ€ä»¥ FastChat åŠ è½½ PEFT è·¯å¾„ã€‚ å¼€å‘è€…éœ€è¦ä¿è¯è·¯å¾„åç§°é‡Œå¿…é¡»æœ‰ peft è¿™ä¸ªè¯ã€‚ é…ç½®æ–‡ä»¶çš„åå­—ä¸º adapter_config.json peft è·¯å¾„ä¸‹åŒ…å«.bin æ ¼å¼çš„ PEFT æƒé‡ï¼Œ peftè·¯å¾„åœ¨startup.pyä¸­ create_model_worker_app å‡½æ•°çš„ args.model_names ä¸­æŒ‡å®š ```python args.model_names = [\"/home/ubuntu/your_peft_folder/peft\"] - æ‰§è¡Œä»£ç ä¹‹ï¼Œåº”è¯¥è®¾å®šç¯å¢ƒå˜é‡ PEFT_SHARE_BASE_WEIGHTS=true æ³¨ï¼šå¦‚æœä¸Šè¿°æ–¹å¼å¯åŠ¨å¤±è´¥ï¼Œåˆ™éœ€è¦ä»¥æ ‡å‡†çš„ FastChat æœåŠ¡å¯åŠ¨æ–¹å¼åˆ†æ­¥å¯åŠ¨ï¼ŒPEFTåŠ è½½è¯¦ç»†æ­¥éª¤å‚è€ƒä»¥ä¸‹ISSUE [åŠ è½½loraå¾®è°ƒåæ¨¡å‹å¤±æ•ˆ](https://github.com/chatchat-space/Langchain-Chatchat/issues/1130#issuecomment-1685291822) åœ¨```æœ€ä½³å®è·µ```ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¸ºå¼€å‘è€…åšäº†æ›´è¯¦ç»†çš„æ¨¡å‹è½½å…¥æ–‡æ¡£ã€‚ __è¯¥åŠŸèƒ½å¯èƒ½è¿˜å…·æœ‰ä¸€å®šçš„Bugï¼Œéœ€è¦å¼€å‘è€…ä»”ç»†é€‚é…ã€‚__ ## ä½¿ç”¨è‡ªå®šä¹‰çš„åµŒå…¥æ¨¡å‹ - ä½¿ç”¨è‡ªå®šä¹‰çš„åµŒå…¥æ¨¡å‹ï¼Œå¼€å‘è€…éœ€è¦å°†å…¶åˆå¹¶åˆ°åŸå§‹çš„åµŒå…¥æ¨¡å‹ä¸­ï¼Œä¹‹åä»…éœ€å°†å…¶è·¯å¾„æ·»åŠ åˆ°```config/model_config.py```ä¸­å¹¶é€‰æ‹©è‡ªå·±çš„æ¨¡å‹å¯åŠ¨å³å¯ã€‚ - å¦‚æœæƒ³è‡ªå·±åœ¨Embeddingæ¨¡å‹ä¸­æ”¯æŒ è‡ªå®šä¹‰çš„å…³é”®å­—ï¼Œéœ€è¦åœ¨ ```embeddings/embedding_keywords.txt```ä¸­è®¾å®šå¥½è‡ªå·±çš„å…³é”®å­— - è¿è¡Œ ```embeddings/add_embedding_keywords.py å°†ç”Ÿæˆçš„æ–°Embeddingæ¨¡å‹åœ°å€æ”¾å…¥configs/model_config.pyä¸­å¹¶é€‰æ‹©ï¼Œ\"custom-embedding\": \"your path\", å¹¶è®¾ç½®EMBEDDING_MODEL = \"custom-embedding\" å³å¯è°ƒç”¨åŠ å…¥å…³é”®å­—çš„embeddingæ¨¡å‹ã€‚ åœ¨æœ€ä½³å®è·µç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¸ºæŸå‡ ä¸ªå…³é”®è¯å®šåˆ¶äº†ä¸€ä¸ªEmbedæ¨¡å‹ã€‚ æ—¥å¿—åŠŸèƒ½ æ—¥å¿—åŠŸèƒ½è®°å½•äº†å¤§æ¨¡å‹çš„å¿ƒè·³å’Œç½‘ç»œç«¯å£ä¼ è¾“è®°å½•ï¼Œå¼€å‘è€…å¯ä»¥é€šè¿‡æ—¥å¿—åŠŸèƒ½æŸ¥çœ‹æ¨¡å‹çš„è¿è¡Œæƒ…å†µã€‚ Â© snowdreams1006 all right reservedï¼Œpowered by Gitbookæ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š 2024-05-17 23:23:32 "},"LLM/æœ€ä½³å®è·µ.html":{"url":"LLM/æœ€ä½³å®è·µ.html","title":"æœ€ä½³å®è·µ","keywords":"","body":"æ¨èçš„æ¨¡å‹ç»„åˆ åœ¨é»˜è®¤çš„é…ç½®æ–‡ä»¶ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹æ¨¡å‹ç»„åˆ LLM: Chatglm2-6b Embedding Models: m3e-base TextSplitter: ChineseRecursiveTextSplitter Kb_dataset: faiss æˆ‘ä»¬æ¨èå¼€å‘è€…æ ¹æ®è‡ªå·±çš„ä¸šåŠ¡éœ€æ±‚è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œå¦‚æœä¸éœ€è¦å¾®è°ƒä¸”é…ç½®å……è¶³ï¼Œå¯é€‰æ‹©ä»¥ä¸‹æ€§èƒ½è¾ƒå¥½çš„é…ç½® model_config.py LLM: Qwen-14B-Chat æˆ– Baichuan2-13B-Chat Embedding Models: piccolo-large-zh æˆ– bge-large-zh-v1.5 HISTORY_LEN = 20 TEMPERATURE = 0.1 ä½¿ç”¨è¯¥æ¨¡å‹å°†éœ€è¦æ›´é«˜çš„ç¡¬ä»¶è¦æ±‚ ``` 1å¼  RTX A6000 æˆ–è€… A40 ç­‰ 48GB æ˜¾å­˜ä»¥ä¸Šçš„æ˜¾å¡ã€‚æ¨è 1 x A100 ä»¥ä¸Šã€‚ (ä½¿ç”¨å¤šå¼ æ˜¾å¡æ‹¼æ¥ä¹Ÿèƒ½è¿è¡Œï¼Œä½†æ˜¯é€Ÿåº¦éå¸¸æ…¢ï¼Œ2å¼ 4090æ‹¼æ¥è¿è¡Œå¤§æ¦‚ä¸ºä¸€ç§’ä¸€ä¸ªå­—çš„é€Ÿåº¦) 64GB å†…å­˜ç”¨äºåŠ è½½æ¨¡å‹è€Œä¸è¢«Kill æœåŠ¡å™¨çº§çš„CPUï¼Œæ¨è Xeon(R) Platinum 8358P ä»¥ä¸Š + å¦‚æœå¼€å‘è€…çŸ¥è¯†åº“è¾ƒå¤§ï¼Œæœ‰å¤§é‡æ–‡æ¡£ï¼Œå¤§æ–‡ä»¶ï¼Œæˆ‘ä»¬æ¨èå¼€å‘è€…ä½¿ç”¨ ```pg``` å‘é‡æ•°æ®åº“ + å¦‚æœå¼€å‘è€…çš„çŸ¥è¯†åº“å…·æœ‰ä¸€å®šçš„å…³é”®è¯ç‰¹å¾ï¼Œä¾‹å¦‚ï¼š + é—®ç­”å¯¹æ–‡ä»¶(ä»¥Q + A ä¸ºä¸€ä¸ªç»„åˆçš„jsonæ–‡ä»¶) + Markdownæ–‡ä»¶ + å¹¶æ’çš„pdfæ–‡ä»¶ + å…·æœ‰å¤šä¸ªè¡¨æ ¼çš„pdfæ–‡ä»¶ æˆ‘ä»¬æ¨èå¼€å‘è€…è‡ªè¡Œå¼€å‘åˆ†è¯å™¨ï¼Œä»¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚ + å¦‚æœå¼€å‘è€…æƒ³ä½¿ç”¨æ›´å…¨é¢çš„ Agent åŠŸèƒ½ï¼Œæˆ‘ä»¬æ¨èå¼€å‘è€…ä½¿ç”¨ä»¥ä¸‹é…ç½® LLM: Qwen-14B-Chat, AgentLM-70B æˆ– GPT-4 Tools çš„å·¥å…·æ§åˆ¶åœ¨10ä¸ªä¹‹å†… ## å¾®è°ƒæ¨¡å‹åŠ è½½å®æ“ ### ép-tuningç±»PEFTåŠ è½½ æœ¬é¡¹ç›®åŸºäº FastChat åŠ è½½ LLM æœåŠ¡ï¼Œæ•…éœ€ä»¥ FastChat åŠ è½½ PEFT è·¯å¾„ï¼Œé’ˆå¯¹chatglm,falconï¼Œcodet5pä»¥å¤–çš„æ¨¡å‹ï¼Œä»¥åŠép-tuningä»¥å¤–çš„peftæ–¹æ³•ï¼Œéœ€å¯¹peftæ–‡ä»¶è¿›è¡Œä¿®æ”¹ï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š 1. å°†config.jsonæ–‡ä»¶ä¿®æ”¹ä¸ºadapter_config.json; 2. ä¿è¯æ–‡ä»¶å¤¹åŒ…å«pytorch_model.binæ–‡ä»¶ï¼› 3. ä¿®æ”¹æ–‡ä»¶å¤¹åç§°ï¼Œä¿è¯æ–‡ä»¶å¤¹åŒ…å«'peft'ä¸€è¯ï¼› 4. å°†peftæ–‡ä»¶å¤¹ç§»å…¥é¡¹ç›®ç›®å½•ä¸‹ï¼› 5. ç¡®ä¿adapter_config.jsonæ–‡ä»¶å¤¹ä¸­base_model_name_or_pathæŒ‡å‘åŸºç¡€æ¨¡å‹ï¼› 6. å°†peftè·¯å¾„æ·»åŠ åˆ°model_config.pyçš„llm_dictä¸­ï¼Œé”®ä¸ºæ¨¡å‹åï¼Œå€¼ä¸ºpeftè·¯å¾„ï¼Œæ³¨æ„ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œå¦‚\"peft\"ï¼› 7. å¼€å¯ `PEFT_SHARE_BASE_WEIGHTS=true`ç¯å¢ƒå˜é‡ï¼Œå†æ‰§è¡Œpython startup.py -a é’ˆå¯¹p-tuningå’Œchatglmæ¨¡å‹ï¼Œéœ€è¦å¯¹fastchatè¿›è¡Œè¾ƒå¤§å¹…åº¦çš„ä¿®æ”¹ã€‚ ### p-tuningåŠ è½½ P-tuningè™½ç„¶æ˜¯ä¸€ç§peftæ–¹æ³•ï¼Œä½†å¹¶ä¸èƒ½äºhuggingfaceçš„peft pythonåŒ…å…¼å®¹ï¼Œè€Œfastchatåœ¨å¤šå¤„ä»¥å­—ç¬¦ä¸²åŒ¹é…çš„æ–¹å¼è¿›è¡Œç¡¬ç¼–ç åŠ è½½æ¨¡å‹ï¼Œå› æ­¤å¯¼è‡´fastchatå’Œchatchatä¸èƒ½å…¼å®¹p-tuningï¼Œç»langchain-chatchatå¼€å‘ç»„å¤šæ¬¡å°è¯•ï¼Œç»™å‡ºå¦‚ä¸‹æŒ‡å—è¿›è¡Œp-tuningåŠ è½½ã€‚ #### 1. peftæ–‡ä»¶å¤¹ä¿®æ”¹ 1. å°†config.jsonæ–‡ä»¶ä¿®æ”¹ä¸ºadapter_config.json; 2. ä¿è¯æ–‡ä»¶å¤¹åŒ…å«pytorch_model.binæ–‡ä»¶ï¼› 3. ä¿®æ”¹æ–‡ä»¶å¤¹åç§°ï¼Œä¿è¯æ–‡ä»¶å¤¹åŒ…å«'peft'ä¸€è¯ï¼› 4. åœ¨adapter_config.jsonæ–‡ä»¶ä¸­å¢åŠ å¦‚ä¸‹å­—æ®µï¼š ```json \"base_model_name_or_path\": \"/root/model/chatglm2-6b/\" \"task_type\": \"CAUSAL_LM\", \"peft_type\": \"PREFIX_TUNING\", \"inference_mode\": true, \"revision\": \"main\", \"num_virtual_tokens\": 16 å…¶ä¸­,\"base_model_name_or_path\"ä¸ºåŸºç¡€æ¨¡å‹çš„å­˜åœ¨ä½ç½®ï¼› å°†æ–‡ä»¶å¤¹ç§»å…¥é¡¹ç›®æ–‡ä»¶å¤¹ä¸­ï¼Œå¦‚Langchain-Chatchaté¡¹ç›®æ–‡ä»¶å¤¹ç›®å½•ä¸‹ï¼› 2. fastchatåŒ…ä»£ç ä¿®æ”¹ 2.1 fastchat.model.model_adapteræ–‡ä»¶ä¿®æ”¹ å°†fastchat.model.model_adapter.pyæ–‡ä»¶çš„load_modelå‡½æ•°ä¿®æ”¹ä¸ºï¼š def load_model( model_path: str, device: str = \"cuda\", num_gpus: int = 1, max_gpu_memory: Optional[str] = None, dtype: Optional[torch.dtype] = None, load_8bit: bool = False, cpu_offloading: bool = False, gptq_config: Optional[GptqConfig] = None, awq_config: Optional[AWQConfig] = None, revision: str = \"main\", debug: bool = False, load_kwargs = {} ): \"\"\"Load a model from Hugging Face.\"\"\" # get model adapter adapter = get_model_adapter(model_path) kwargs = load_kwargs # Handle device mapping cpu_offloading = raise_warning_for_incompatible_cpu_offloading_configuration( device, load_8bit, cpu_offloading ) if device == \"cpu\": kwargs[\"torch_dtype\"]= torch.float32 if CPU_ISA in [\"avx512_bf16\", \"amx\"]: try: import intel_extension_for_pytorch as ipex kwargs [\"torch_dtype\"]= torch.bfloat16 except ImportError: warnings.warn( \"Intel Extension for PyTorch is not installed, it can be installed to accelerate cpu inference\" ) elif device == \"cuda\": kwargs[\"torch_dtype\"] = torch.float16 if num_gpus != 1: kwargs[\"device_map\"] = \"auto\" if max_gpu_memory is None: kwargs[ \"device_map\" ] = \"sequential\" # This is important for not the same VRAM sizes available_gpu_memory = get_gpu_memory(num_gpus) kwargs[\"max_memory\"] = { i: str(int(available_gpu_memory[i] * 0.85)) + \"GiB\" for i in range(num_gpus) } else: kwargs[\"max_memory\"] = {i: max_gpu_memory for i in range(num_gpus)} elif device == \"mps\": kwargs[\"torch_dtype\"] = torch.float16 # Avoid bugs in mps backend by not using in-place operations. replace_llama_attn_with_non_inplace_operations() elif device == \"xpu\": kwargs[\"torch_dtype\"] = torch.bfloat16 # Try to load ipex, while it looks unused, it links into torch for xpu support try: import intel_extension_for_pytorch as ipex except ImportError: warnings.warn( \"Intel Extension for PyTorch is not installed, but is required for xpu inference.\" ) elif device == \"npu\": kwargs[\"torch_dtype\"]= torch.float16 # Try to load ipex, while it looks unused, it links into torch for xpu support try: import torch_npu except ImportError: warnings.warn(\"Ascend Extension for PyTorch is not installed.\") else: raise ValueError(f\"Invalid device: {device}\") if cpu_offloading: # raises an error on incompatible platforms from transformers import BitsAndBytesConfig if \"max_memory\" in kwargs: kwargs[\"max_memory\"][\"cpu\"] = ( str(math.floor(psutil.virtual_memory().available / 2**20)) + \"Mib\" ) kwargs[\"quantization_config\"] = BitsAndBytesConfig( load_in_8bit_fp32_cpu_offload=cpu_offloading ) kwargs[\"load_in_8bit\"] = load_8bit elif load_8bit: if num_gpus != 1: warnings.warn( \"8-bit quantization is not supported for multi-gpu inference.\" ) else: model, tokenizer = adapter.load_compress_model( model_path=model_path, device=device, torch_dtype=kwargs[\"torch_dtype\"], revision=revision, ) if debug: print(model) return model, tokenizer elif awq_config and awq_config.wbits å°†fastchat.model.model_adapter.pyçš„å‡½æ•°ä¿®æ”¹ä¸ºï¼š def get_generate_stream_function(model: torch.nn.Module, model_path: str): \"\"\"Get the generate_stream function for inference.\"\"\" from fastchat.serve.inference import generate_stream model_type = str(type(model)).lower() is_chatglm = \"chatglm\" in model_type is_falcon = \"rwforcausallm\" in model_type is_codet5p = \"codet5p\" in model_type is_peft = \"peft\" in model_type if is_chatglm: return generate_stream_chatglm elif is_falcon: return generate_stream_falcon elif is_codet5p: return generate_stream_codet5p elif peft_share_base_weights and is_peft: # Return a curried stream function that loads the right adapter # according to the model_name available in this context. This ensures # the right weights are available. @torch.inference_mode() def generate_stream_peft( model, tokenizer, params: Dict, device: str, context_len: int, stream_interval: int = 2, judge_sent_end: bool = False, ): model.set_adapter(model_path) if \"chatglm\" in str(type(model.base_model)).lower(): model.disable_adapter() prefix_state_dict = torch.load(os.path.join(model_path, \"pytorch_model.bin\")) new_prefix_state_dict = {} for k, v in prefix_state_dict.items(): if k.startswith(\"transformer.prefix_encoder.\"): new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v elif k.startswith(\"transformer.prompt_encoder.\"): new_prefix_state_dict[k[len(\"transformer.prompt_encoder.\"):]] = v model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict) for x in generate_stream_chatglm( model, tokenizer, params, device, context_len, stream_interval, judge_sent_end, ): yield x elif \"rwforcausallm\" in str(type(model.base_model)).lower(): for x in generate_stream_falcon( model, tokenizer, params, device, context_len, stream_interval, judge_sent_end, ): yield x elif \"codet5p\" in str(type(model.base_model)).lower(): for x in generate_stream_codet5p( model, tokenizer, params, device, context_len, stream_interval, judge_sent_end, ): yield x else: for x in generate_stream( model, tokenizer, params, device, context_len, stream_interval, judge_sent_end, ): yield x return generate_stream_peft else: return generate_stream å°†fastchat.model.model_adapter.pyçš„PeftModelAdapterç±»çš„load_modelæ–¹æ³•ä¿®æ”¹ä¸ºï¼š def load_model(self, model_path: str, from_pretrained_kwargs: dict): \"\"\"Loads the base model then the (peft) adapter weights\"\"\" from peft import PeftConfig, PeftModel config = PeftConfig.from_pretrained(model_path) base_model_path = config.base_model_name_or_path if \"peft\" in base_model_path: raise ValueError( f\"PeftModelAdapter cannot load a base model with 'peft' in the name: {config.base_model_name_or_path}\" ) # Basic proof of concept for loading peft adapters that share the base # weights. This is pretty messy because Peft re-writes the underlying # base model and internally stores a map of adapter layers. # So, to make this work we: # 1. Cache the first peft model loaded for a given base models. # 2. Call `load_model` for any follow on Peft models. # 3. Make sure we load the adapters by the model_path. Why? This is # what's accessible during inference time. # 4. In get_generate_stream_function, make sure we load the right # adapter before doing inference. This *should* be safe when calls # are blocked the same semaphore. if peft_share_base_weights: if base_model_path in peft_model_cache: model, tokenizer = peft_model_cache[base_model_path] # Super important: make sure we use model_path as the # `adapter_name`. model.load_adapter(model_path, adapter_name=model_path) else: base_adapter = get_model_adapter(base_model_path) base_model, tokenizer = base_adapter.load_model( base_model_path, from_pretrained_kwargs ) # Super important: make sure we use model_path as the # `adapter_name`. from peft import get_peft_model model = get_peft_model(base_model,config,adapter_name=model_path) peft_model_cache[base_model_path] = (model, tokenizer) return model, tokenizer # In the normal case, load up the base model weights again. base_adapter = get_model_adapter(base_model_path) base_model, tokenizer = base_adapter.load_model( base_model_path, from_pretrained_kwargs ) from peft import get_peft_model model = get_peft_model(base_model,config,adapter_name=model_path) return model, tokenizer å°†fastchat.model.model_adapter.pyçš„ChatglmAdapterç±»çš„load_modelæ–¹æ³•ä¿®æ”¹ä¸ºï¼š def load_model(self, model_path: str, from_pretrained_kwargs: dict): revision = from_pretrained_kwargs.get(\"revision\", \"main\") tokenizer = AutoTokenizer.from_pretrained( model_path, trust_remote_code=True, revision=revision ) config = AutoConfig.from_pretrained(model_path, trust_remote_code=True,**from_pretrained_kwargs) model = AutoModel.from_pretrained( model_path, trust_remote_code=True, config=config ) return model, tokenizer 2.2 fastchat.serve.model_workeræ–‡ä»¶ä¿®æ”¹ å°†fastchat.serve.modelworkeræ–‡ä»¶çš„ModelWorkerçš„_initæ–¹æ³•ä¿®æ”¹å¦‚ä¸‹ï¼š class ModelWorker(BaseModelWorker): def __init__( self, controller_addr: str, worker_addr: str, worker_id: str, model_path: str, model_names: List[str], limit_worker_concurrency: int, no_register: bool, device: str, num_gpus: int, max_gpu_memory: str, dtype: Optional[torch.dtype] = None, load_8bit: bool = False, cpu_offloading: bool = False, gptq_config: Optional[GptqConfig] = None, awq_config: Optional[AWQConfig] = None, stream_interval: int = 2, conv_template: Optional[str] = None, embed_in_truncate: bool = False, seed: Optional[int] = None, load_kwargs = {}, #ä¿®æ”¹ç‚¹ **kwargs, ): super().__init__( controller_addr, worker_addr, worker_id, model_path, model_names, limit_worker_concurrency, conv_template=conv_template, ) logger.info(f\"Loading the model {self.model_names} on worker {worker_id} ...\") self.model, self.tokenizer = load_model( model_path, device=device, num_gpus=num_gpus, max_gpu_memory=max_gpu_memory, dtype=dtype, load_8bit=load_8bit, cpu_offloading=cpu_offloading, gptq_config=gptq_config, awq_config=awq_config, load_kwargs=load_kwargs #ä¿®æ”¹ç‚¹ ) self.device = device if self.tokenizer.pad_token == None: self.tokenizer.pad_token = self.tokenizer.eos_token self.context_len = get_context_length(self.model.config) print(\"**\"*100) self.generate_stream_func = get_generate_stream_function(self.model, model_path) print(f\"self.generate_stream_func{self.generate_stream_func}\") print(\"*\"*100) self.stream_interval = stream_interval self.embed_in_truncate = embed_in_truncate self.seed = seed if not no_register: self.init_heart_beat() åœ¨fastchat.serve.model_workeræ–‡ä»¶çš„create_model_workerå¢åŠ å¦‚ä¸‹argså‚æ•°ï¼š parser.add_argument(\"--load_kwargs\",type=dict,default={}) å¹¶å°†å¦‚ä¸‹è¯­å¥ï¼š worker = ModelWorker( args.controller_address, args.worker_address, worker_id, args.model_path, args.model_names, args.limit_worker_concurrency, no_register=args.no_register, device=args.device, num_gpus=args.num_gpus, max_gpu_memory=args.max_gpu_memory, dtype=str_to_torch_dtype(args.dtype), load_8bit=args.load_8bit, cpu_offloading=args.cpu_offloading, gptq_config=gptq_config, awq_config=awq_config, stream_interval=args.stream_interval, conv_template=args.conv_template, embed_in_truncate=args.embed_in_truncate, seed=args.seed, ) ä¿®æ”¹ä¸ºï¼š worker = ModelWorker( args.controller_address, args.worker_address, worker_id, args.model_path, args.model_names, args.limit_worker_concurrency, no_register=args.no_register, device=args.device, num_gpus=args.num_gpus, max_gpu_memory=args.max_gpu_memory, dtype=str_to_torch_dtype(args.dtype), load_8bit=args.load_8bit, cpu_offloading=args.cpu_offloading, gptq_config=gptq_config, awq_config=awq_config, stream_interval=args.stream_interval, conv_template=args.conv_template, embed_in_truncate=args.embed_in_truncate, seed=args.seed, load_kwargs=args.load_kwargs ) è‡³æ­¤ï¼Œæˆ‘ä»¬å®Œæˆäº†fastchatåŠ è½½ptuningçš„æ‰€æœ‰ä¿®æ”¹ï¼Œåœ¨è°ƒç”¨fastchatåŠ è½½p-tuningæ—¶ï¼Œå¯ä»¥é€šè¿‡åŠ å…¥ PEFT_SHARE_BASE_WEIGHTS=trueï¼Œå¹¶ä»¥å­—å…¸çš„å½¢å¼æ·»åŠ --load_kwargså‚æ•°ä¸ºè®­ç»ƒptuningæ—¶çš„pre_seq_lenå€¼å³å¯ï¼Œä¾‹å¦‚å°†2.2.2æ­¥éª¤ä¸­çš„ parser.add_argument(\"--load_kwargs\",type=dict,default={})ä¿®æ”¹ä¸ºï¼š parser.add_argument(\"--load_kwargs\",type=dict,default={\"pre_seq_len\":16}) 3 langchain-chatchatä»£ç ä¿®æ”¹ï¼š åœ¨configs/serve_config.pyä¸­çš„FSCHAT_MODEL_WORKERSå­—å…¸ä¸­å¢åŠ å¦‚ä¸‹å­—æ®µï¼š \"load_kwargs\": {\"pre_seq_len\": 16} #å€¼ä¿®æ”¹ä¸ºadapter_config.jsonä¸­çš„pre_seq_lenå€¼ å°†startup.pyä¸­çš„create_model_worker_appä¿®æ”¹ä¸ºï¼š ```python def create_model_worker_app(log_level: str = \"INFO\", **kwargs) -> FastAPI: \"\"\" kwargsåŒ…å«çš„å­—æ®µå¦‚ä¸‹ï¼š host: port: model_names:[`model_name`] controller_address: worker_address: å¯¹äºonline_api: online_api:True worker_class: `provider` å¯¹äºç¦»çº¿æ¨¡å‹ï¼š model_path: `model_name_or_path`,huggingfaceçš„repo-idæˆ–æœ¬åœ°è·¯å¾„ device:`LLM_DEVICE` \"\"\" import fastchat.constants fastchat.constants.LOGDIR = LOG_PATH from fastchat.serve.model_worker import worker_id, logger import argparse logger.setLevel(log_level) parser = argparse.ArgumentParser() args = parser.parse_args([]) for k, v in kwargs.items(): setattr(args, k, v) # åœ¨çº¿æ¨¡å‹API if worker_class := kwargs.get(\"worker_class\"): from fastchat.serve.model_worker import app worker = worker_class(model_names=args.model_names, controller_addr=args.controller_address, worker_addr=args.worker_address) sys.modules[\"fastchat.serve.model_worker\"].worker = worker # æœ¬åœ°æ¨¡å‹ else: from configs.model_config import VLLM_MODEL_DICT if kwargs[\"model_names\"][0] in VLLM_MODEL_DICT and args.infer_turbo == \"vllm\": import fastchat.serve.vllm_worker from fastchat.serve.vllm_worker import VLLMWorker,app from vllm import AsyncLLMEngine from vllm.engine.arg_utils import AsyncEngineArgs,EngineArgs args.tokenizer = args.model_path # å¦‚æœtokenizerä¸model_pathä¸ä¸€è‡´åœ¨æ­¤å¤„æ·»åŠ  args.tokenizer_mode = 'auto' args.trust_remote_code= True args.download_dir= None args.load_format = 'auto' args.dtype = 'auto' args.seed = 0 args.worker_use_ray = False args.pipeline_parallel_size = 1 args.tensor_parallel_size = 1 args.block_size = 16 args.swap_space = 4 # GiB args.gpu_memory_utilization = 0.90 args.max_num_batched_tokens = 2560 args.max_num_seqs = 256 args.disable_log_stats = False args.conv_template = None args.limit_worker_concurrency = 5 args.no_register = False args.num_gpus = 1 # vllm workerçš„åˆ‡åˆ†æ˜¯tensorå¹¶è¡Œï¼Œè¿™é‡Œå¡«å†™æ˜¾å¡çš„æ•°é‡ args.engine_use_ray = False args.disable_log_requests = False if args.model_path: args.model = args.model_path if args.num_gpus > 1: args.tensor_parallel_size = args.num_gpus for k, v in kwargs.items(): setattr(args, k, v) engine_args = AsyncEngineArgs.from_cli_args(args) engine = AsyncLLMEngine.from_engine_args(engine_args) worker = VLLMWorker( controller_addr = args.controller_address, worker_addr = args.worker_address, worker_id = worker_id, model_path = args.model_path, model_names = args.model_names, limit_worker_concurrency = args.limit_worker_concurrency, no_register = args.no_register, llm_engine = engine, conv_template = args.conv_template, ) sys.modules[\"fastchat.serve.vllm_worker\"].engine = engine sys.modules[\"fastchat.serve.vllm_worker\"].worker = worker else: from fastchat.serve.model_worker import app, GptqConfig, AWQConfig, ModelWorker args.gpus = \"0\" # GPUçš„ç¼–å·,å¦‚æœæœ‰å¤šä¸ªGPUï¼Œå¯ä»¥è®¾ç½®ä¸º\"0,1,2,3\" args.max_gpu_memory = \"20GiB\" args.num_gpus = 1 # model workerçš„åˆ‡åˆ†æ˜¯modelå¹¶è¡Œï¼Œè¿™é‡Œå¡«å†™æ˜¾å¡çš„æ•°é‡ args.load_8bit = False args.cpu_offloading = None args.gptq_ckpt = None args.gptq_wbits = 16 args.gptq_groupsize = -1 args.gptq_act_order = False args.awq_ckpt = None args.awq_wbits = 16 args.awq_groupsize = -1 args.model_names = [] args.conv_template = None args.limit_worker_concurrency = 5 args.stream_interval = 2 args.no_register = False args.embed_in_truncate = False args.load_kwargs = {\"pre_seq_len\": 16} # æ”¹************************* for k, v in kwargs.items(): setattr(args, k, v) if args.gpus: if args.num_gpus is None: args.num_gpus = len(args.gpus.split(',')) if len(args.gpus.split(\",\")) è‡³æ­¤ï¼Œæˆ‘ä»¬å®Œæˆäº†langchain-chatchatåŠ è½½p-tuningçš„å…¨éƒ¨æ“ä½œï¼Œå°†ptuingçš„è·¯å¾„æ·»åŠ åˆ°model_configçš„llm_dictï¼Œå¦‚ chatglm2-6b: 'p-tuning-peft' å³å¯ä»¥å¦‚ä¸‹æ–¹å¼åŠ è½½p-tuningï¼š ```shell PEFT_SHARE_BASE_WEIGHTS=true python startup.py -a é¢„å¤„ç†çŸ¥è¯†åº“æ–‡ä»¶ åœ¨è½½å…¥çŸ¥è¯†åº“æ–‡ä»¶çš„æ—¶å€™ï¼Œç›´æ¥ä¸Šä¼ æ–‡æ¡£è™½ç„¶èƒ½å®ç°åŸºç¡€çš„é—®ç­”ï¼Œä½†æ˜¯ï¼Œå…¶æ•ˆæœå¹¶ä¸èƒ½å‘æŒ¥åˆ°æœ€ä½³æ°´å¹³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å»ºè®®å¼€å‘è€…å¯¹çŸ¥è¯†åº“æ–‡ä»¶åšå‡ºä»¥ä¸‹çš„é¢„å¤„ç†ã€‚ ä»¥ä¸‹æ–¹å¼çš„é¢„å¤„ç†å¦‚æœæ‰§è¡Œäº†ï¼Œæœ‰æ¦‚ç‡æå‡æ¨¡å‹çš„å¬å›ç‡ã€‚ 1. ä½¿ç”¨TXT / Markdown ç­‰æ ¼å¼åŒ–æ–‡ä»¶ï¼Œå¹¶æŒ‰ç…§è¦ç‚¹æ’ç‰ˆ ä¾‹å¦‚ï¼Œä»¥ä¸‹æ®µè½åº”è¯¥è¢«å¤„ç†æˆå¦‚ä¸‹å†…å®¹ååœ¨åµŒå…¥çŸ¥è¯†åº“ï¼Œä¼šæœ‰æ›´å¥½çš„æ•ˆæœã€‚ åŸæ–‡: PDFç±»å‹ æŸ¥ç‰¹æŸ¥ç‰¹å›¢é˜Ÿè£è·AGI Playground Hackathoné»‘å®¢æ¾â€œç”Ÿäº§åŠ›å·¥å…·çš„æ–°æƒ³è±¡â€èµ›é“å­£å†› 2023å¹´10æœˆ16æ—¥, Founder Parkåœ¨è¿‘æ—¥ç»“æŸçš„AGI Playground Hackathoné»‘å®¢æ¾æ¯”èµ›ä¸­ï¼ŒæŸ¥ç‰¹æŸ¥ç‰¹å›¢é˜Ÿå±•ç°å‡ºè‰²çš„å®åŠ›ï¼Œè£è·äº†â€œç”Ÿäº§åŠ›å·¥å…·çš„æ–°æƒ³è±¡â€èµ›é“å­£å†›ã€‚æœ¬æ¬¡æ¯”èµ›ç”±Founder Parkä¸»åŠï¼Œå¹¶ç”±æ™ºè°±ã€Difyã€Zillizã€å£°ç½‘ã€AWSäº‘æœåŠ¡ç­‰ä¼ä¸šååŠã€‚ æ¯”èµ›å¸å¼•äº†120å¤šæ”¯å‚èµ›å›¢é˜Ÿï¼Œæœ€ç»ˆæœ‰36æ”¯é˜Ÿä¼è¿›å…¥å†³èµ›ï¼Œå…¶ä¸­34æ”¯é˜Ÿä¼æˆåŠŸå®Œæˆäº†è·¯æ¼”ã€‚æ¯”èµ›è§„å®šï¼Œæ‰€æœ‰å‚èµ›é€‰æ‰‹å¿…é¡»åœ¨çŸ­çŸ­çš„48å°æ—¶å†…å®Œæˆä¸€ä¸ªåº”ç”¨äº§å“å¼€å‘ï¼ŒåŒæ—¶è¦æ±‚ä½¿ç”¨æ™ºè°±å¤§æ¨¡å‹åŠZillizå‘é‡æ•°æ®åº“è¿›è¡Œå¼€å‘ã€‚ æŸ¥ç‰¹æŸ¥ç‰¹å›¢é˜Ÿçš„ç°åœºå‚èµ›äººå‘˜ç”±ä¸¤åé¡¹ç›®æˆå‘˜ç»„æˆï¼š æ¥è‡ªAå¤§å­¦çš„å°æ˜è´Ÿè´£äº†Agentæ—…æ¸¸åŠ©æ‰‹çš„å¼€å‘ã€åœºåœ°åè°ƒä»¥åŠå›¢é˜Ÿä½å®¿å’Œè¡Œç¨‹çš„å®‰æ’ï¼›åœ¨ä¿è¯å›¢é˜Ÿå®Œèµ›ä¸Šåšå‡ºäº†ä¸»è¦è´¡çŒ®ã€‚ä½œä¸ºé˜Ÿé•¿ï¼Œæ ‹å®‡åšæŒè‡ªä¿¡ï¼Œåˆ›æ–°ï¼Œæ²‰ç€çš„ç²¾ç¥ï¼Œä¸æ–­æå‡ºæ”¹è¿›æ–¹æ¡ˆå¹¶æŠ“ç´§è½å®ï¼Œé‡åˆ°ç›¸å…³é—®é¢˜ç§¯æè¯·æ•™è€å¸ˆï¼Œæé«˜äº†å›¢é˜Ÿå¼€å‘æ•ˆç‡ã€‚ ä½œä¸ºæ ¸å¿ƒå¼€å‘è€…çš„Bå…¬å¸å°è“ï¼Œä»–åˆ™ä¸»ç®¡Agentæ™ºèƒ½çŸ¥è¯†åº“æŸ¥è¯¢å¼€å‘ã€Agentåº•å±‚æ¡†æ¶è®¾è®¡ã€ç›¸å…³APIè°ƒæ•´å’ŒUIè°ƒæ•´ã€‚åœ¨æœ€åï¼Œä»–ä»£è¡¨å›¢é˜Ÿåœ¨è§„å®šçš„æ—¶é—´å†…å‘ˆç°äº†äº§å“çš„ç‰¹ç‚¹å’Œä¼˜åŠ¿ï¼Œå¹¶å®Œç¾çš„å±•ç¤ºäº†äº§å“demoã€‚ä¸ºå›¢é˜Ÿæœ€ç»ˆäº§å“èƒ½å¤Ÿå¾—åˆ°å¥–é¡¹åšå‡ºäº†é‡è¦è´¡çŒ®ã€‚ ä¿®æ”¹åçš„Markdownæ–‡ä»¶ï¼Œå…·æœ‰æ›´é«˜çš„å¬å›ç‡ # æŸ¥ç‰¹æŸ¥ç‰¹å›¢é˜Ÿè£è·AGI Playground Hackathoné»‘å®¢æ¾â€œç”Ÿäº§åŠ›å·¥å…·çš„æ–°æƒ³è±¡â€èµ›é“å­£å†›ã€‚ ## æŠ¥é“ç®€ä»‹ 2023å¹´10æœˆ16æ—¥, Founder Parkåœ¨è¿‘æ—¥ç»“æŸçš„AGI Playground Hackathoné»‘å®¢æ¾æ¯”èµ›ä¸­ï¼ŒæŸ¥ç‰¹æŸ¥ç‰¹å›¢é˜Ÿå±•ç°å‡ºè‰²çš„å®åŠ›ï¼Œè£è·äº†â€œç”Ÿäº§åŠ›å·¥å…·çš„æ–°æƒ³è±¡â€èµ›é“å­£å†›ã€‚æœ¬æ¬¡æ¯”èµ›ç”±Founder Parkä¸»åŠï¼Œå¹¶ç”±æ™ºè°±ã€Difyã€Zillizã€å£°ç½‘ã€AWSäº‘æœåŠ¡ç­‰ä¼ä¸šååŠã€‚ ## æ¯”èµ›ä»‹ç» æ¯”èµ›å¸å¼•äº†120å¤šæ”¯å‚èµ›å›¢é˜Ÿï¼Œæœ€ç»ˆæœ‰36æ”¯é˜Ÿä¼è¿›å…¥å†³èµ›ï¼Œå…¶ä¸­34æ”¯é˜Ÿä¼æˆåŠŸå®Œæˆäº†è·¯æ¼”ã€‚æ¯”èµ›è§„å®šï¼Œæ‰€æœ‰å‚èµ›é€‰æ‰‹å¿…é¡»åœ¨çŸ­çŸ­çš„48å°æ—¶å†…å®Œæˆä¸€ä¸ªåº”ç”¨äº§å“å¼€å‘ï¼ŒåŒæ—¶è¦æ±‚ä½¿ç”¨æ™ºè°±å¤§æ¨¡å‹åŠZillizå‘é‡æ•°æ®åº“è¿›è¡Œå¼€å‘ã€‚ ## è·å¥–é˜Ÿå‘˜ç®€ä»‹ + å°æ˜ï¼ŒAå¤§å­¦ + è´Ÿè´£Agentæ—…æ¸¸åŠ©æ‰‹çš„å¼€å‘ã€åœºåœ°åè°ƒä»¥åŠå›¢é˜Ÿä½å®¿å’Œè¡Œç¨‹çš„å®‰æ’ + åœ¨ä¿è¯å›¢é˜Ÿå®Œèµ›ä¸Šåšå‡ºäº†ä¸»è¦è´¡çŒ®ã€‚ä½œä¸ºé˜Ÿé•¿ï¼Œæ ‹å®‡åšæŒè‡ªä¿¡ï¼Œåˆ›æ–°ï¼Œæ²‰ç€çš„ç²¾ç¥ï¼Œä¸æ–­æå‡ºæ”¹è¿›æ–¹æ¡ˆå¹¶æŠ“ç´§è½å®ï¼Œé‡åˆ°ç›¸å…³é—®é¢˜ç§¯æè¯·æ•™è€å¸ˆï¼Œæé«˜äº†å›¢é˜Ÿå¼€å‘æ•ˆç‡ã€‚ + å°è“ï¼ŒBå…¬å¸ + ä¸»ç®¡Agentæ™ºèƒ½çŸ¥è¯†åº“æŸ¥è¯¢å¼€å‘ã€Agentåº•å±‚æ¡†æ¶è®¾è®¡ã€ç›¸å…³APIè°ƒæ•´å’ŒUIè°ƒæ•´ã€‚ + ä»£è¡¨å›¢é˜Ÿåœ¨è§„å®šçš„æ—¶é—´å†…å‘ˆç°äº†äº§å“çš„ç‰¹ç‚¹å’Œä¼˜åŠ¿ï¼Œå¹¶å®Œç¾çš„å±•ç¤ºäº†äº§å“demoã€‚ 2. å‡å°‘æ–‡ä»¶ä¸­å†²çªçš„å†…å®¹ï¼Œåˆ†é—¨åˆ«ç±»å­˜æ”¾æ•°æ® å°±åƒäººç±»å¯»æ‰¾ç›¸å…³ç‚¹ä¸€æ ·ï¼Œå¦‚æœåœ¨å¤šä»½æ–‡ä»¶ä¸­å­˜åœ¨ç›¸ä¼¼çš„å†…å®¹ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹æ— æ³•å‡†ç¡®çš„æœç´¢åˆ°ç›¸å…³å†…å®¹ã€‚ å› æ­¤ï¼Œéœ€è¦å‡å°‘æ–‡ä»¶ä¸­ç›¸ä¼¼çš„å†…å®¹ï¼Œæˆ–å°†å…¶åˆ†åœ¨ä¸åŒçš„çŸ¥è¯†åº“ä¸­ã€‚ ä¾‹å¦‚ï¼Œä»¥ä¸‹ä¸¤ä¸ªå¥å­ä¸­ï¼Œå¦‚æœæœç´¢å¤–ç±æ•™å¸ˆï¼Œåˆ™å…·æœ‰æ­§ä¹‰ï¼Œéå¸¸å®¹æ˜“æœç´¢åˆ°é”™è¯¯ç­”æ¡ˆã€‚ æ–‡ä»¶ä¸€ï¼š åœ¨å¤§æ•°æ®ä¸“ä¸šä¸­ï¼Œæˆ‘ä»¬å·²ç»æ‹¥æœ‰è¶…è¿‡1/3çš„å¤–ç±åšå£«å’Œæ•™å¸ˆã€‚ æ–‡ä»¶äºŒï¼š æœ¬ä¸“ä¸šå…·æœ‰40%çš„å¤–ç±æ•™å¸ˆæ¯”ä¾‹ï¼Œ æœ¬ä¸“ä¸šæœ‰åšå£«ç”Ÿ10äººï¼Œç ”ç©¶ç”Ÿ12äººã€‚ 3. å‡å°‘å…·æœ‰æ­§ä¹‰çš„å¥å­ çŸ¥è¯†åº“ä¸­åº”è¯¥å‡å°‘å…·æœ‰æ­§ä¹‰çš„å¥å­å’Œæ®µè½ï¼Œæˆ–è€…æ±‰è¯­çš„é«˜çº§ç”¨æ³•ï¼Œä¾‹å¦‚ 1. ä»–è¯´ä»–ä¼šæ€äº†é‚£ä¸ªäººã€‚ 2. ä½ è¯´å•¥å­ï¼Ÿ 3. æˆ‘å–œæ¬¢ä½ çš„å¤´å‘ã€‚ 4. åœ°æ¿çœŸçš„æ»‘ï¼Œæˆ‘å·®ç‚¹æ²¡æ‘”å€’ã€‚ åœ¨ç›¸ä¼¼åº¦æ¨¡å‹å¯¹æ¯”çš„æ—¶å€™ï¼Œä»…ä»…èƒ½æœç´¢å¥å­çš„è¡¨é¢æ„æ€ï¼Œå› æ­¤ï¼Œä½¿ç”¨æœ‰æ­§ä¹‰çš„å¥å­å’Œæ®µè½å¯èƒ½å¯¼è‡´æœç´¢é”™è¯¯ã€‚ 4. å‡å°‘å•ä¸ªæ–‡ä»¶çš„å¤§å°ï¼Œå‡å°‘æ–‡ä»¶ä¸­çš„ç‰¹æ®Šç¬¦å· ä¸Šä¼ çŸ¥è¯†åº“çš„å•ä¸ªæ–‡ä»¶ä¸å»ºè®®è¶…è¿‡5MBï¼Œä»¥å…å‡ºç°å‘é‡åŒ–ä¸­æ–­å¡æ­»ç­‰æƒ…å†µã€‚åŒæ—¶ï¼Œä¸Šä¼ å¤§æ–‡ä»¶ä¸è¦ä½¿ç”¨faissæ•°æ®åº“ã€‚ å‡å°‘ä¸Šä¼ æ–‡ä»¶ä¸­çš„ä¸­æ–‡ç¬¦å·ï¼Œç‰¹æ®Šç¬¦å·ï¼Œæ— æ„ä¹‰ç©ºæ ¼ç­‰ã€‚ è‡ªå®šä¹‰çš„å…³é”®è¯è°ƒæ•´Embeddingæ¨¡å‹ 1.é¦–å…ˆå‡†å¤‡ä¸€ä¸ªå…³é”®å­—çš„æ–‡æœ¬æ–‡ä»¶ï¼Œæ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªå…³é”®å­—ã€‚ä¾‹å¦‚ï¼š æ–‡ä»¶key_words.txtï¼š iphone13pro ä¸­çŸ³æ²¹ é…ç½®kb_config.pyEMBEDDING_KEYWORD_FILE = \"embedding_keywords.txt\" è¿è¡Œembeddings/add_embedding_keywords.py ``` è¾“å…¥çš„æ–‡æœ¬ï¼ˆè¿™é‡Œåªæ˜¯ä¸€ä¸ªæ²¡åˆ†éš”çš„ä¸€ä¸²å­—ç¬¦ï¼‰ï¼šiphone13pro ç”Ÿæˆçš„token idåºåˆ—ï¼š[101, 21128, 102] tokenåˆ°token idçš„æ˜ å°„ï¼š [CLS]->101 iphone13pro->21128 [SEP]->102 è¾“å…¥çš„æ–‡æœ¬ï¼šä¸­çŸ³æ²¹ ç”Ÿæˆçš„token idåºåˆ—ï¼š[101, 21129, 102] tokenåˆ°token idçš„æ˜ å°„ï¼š [CLS]->101 ä¸­çŸ³æ²¹->21129 [SEP]->102 ``` è¿™æ ·ï¼Œä½ å°±è·å¾—äº†ä¸€ä¸ªæ–°çš„å¸¦æœ‰å…³é”®è¯è°ƒæ•´çš„Embeddingæ¨¡å‹ å®é™…ä½¿ç”¨æ•ˆæœ åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æ”¾ç½®äº†ä¸€äº›æˆåŠŸè°ƒç”¨çš„æ•ˆæœå›¾ï¼Œæ–¹ä¾¿å¼€å‘è€…è¿›è¡ŒæŸ¥çœ‹è‡ªå·±æ˜¯å¦æˆåŠŸè¿è¡Œäº†æ¡†æ¶ã€‚ æ£€æŸ¥æ˜¯å¦æˆåŠŸä¸Šä¼ /ç®¡ç†è‡ªå·±çš„çŸ¥è¯†åº“ åœ¨WebUIç•Œé¢ä¸Šä¼ çŸ¥è¯†åº“ï¼Œåˆ™å¿…é¡»ä¿è¯çŸ¥è¯†åº“è¿›è¡Œå‘é‡åŒ–ï¼ŒæˆåŠŸä¹‹åï¼Œæ–‡ä»¶ä¼šè¢«åˆ‡åˆ†å¹¶åœ¨å‘é‡ä½ç½®æ‰“é’©ã€‚ ä¸‹å›¾å±•ç¤ºäº†æˆåŠŸä¸Šä¼ çŸ¥è¯†åº“çš„ç”»é¢ è¯·ç¡®ä¿æ‰€æœ‰çŸ¥è¯†åº“éƒ½å·²ç»è¿›è¡Œäº†å‘é‡åŒ–ã€‚ æ£€æŸ¥æ˜¯å¦æˆåŠŸå¼€å¯LLMå¯¹è¯ è‹¥æ‰“å¼€webuiåï¼Œåœ¨è¯¥æ¨¡å¼ä¸‹èƒ½æˆåŠŸè·Ÿå¤§æ¨¡å‹å¯¹è¯å³æˆåŠŸè°ƒç”¨ã€‚ ä¸‹å›¾ä¸ºæˆåŠŸè°ƒç”¨LLMçš„æ•ˆæœå›¾: æ£€æŸ¥æ˜¯å¦æˆåŠŸè°ƒç”¨çŸ¥è¯†åº“/æœç´¢ è‹¥æˆåŠŸè°ƒç”¨çŸ¥è¯†åº“ï¼Œåˆ™ä½ åº”è¯¥èƒ½çœ‹åˆ°ï¼Œåœ¨å¤§æ¨¡å‹å›ç­”çš„ä¸‹æ–¹æœ‰ä¸€ä¸ªçŸ¥è¯†åº“åŒ¹é…ç»“æœçš„å±•å¼€æ¡†ï¼Œå¹¶ä¸”å†…éƒ¨æ˜¾ç¤ºäº†ç›¸å…³çš„åŒ¹é…ç»“æœã€‚ å¦‚æœæ²¡æœ‰æœç´¢åˆ°ç›¸å…³å†…å®¹ï¼Œåˆ™ä¼šæç¤ºæ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”é—®é¢˜,å¹¶ä¸”ä¸‹æ‹‰æ¡†ä¸­æ²¡æœ‰ä»»ä½•å†…å®¹ã€‚ ä¸‹å›¾ä¸ºæˆåŠŸè°ƒç”¨çŸ¥è¯†åº“æ•ˆæœå›¾ï¼š åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œç¬¬ä¸€æ¬¡ç”¨æˆ·çš„æé—®æ— æ³•åœ¨çŸ¥è¯†åº“ä¸­å¯»æ‰¾åˆ°åˆé€‚çš„ç­”æ¡ˆï¼Œå› æ­¤ï¼Œå¤§æ¨¡å‹å›ç­”äº†æ ¹æ®å·²çŸ¥ä¿¡æ¯æ— æ³•å›ç­”é—®é¢˜ã€‚ ç¬¬äºŒæ¬¡ç”¨æˆ·çš„æé—®èƒ½åœ¨çŸ¥è¯†åº“ä¸­å¯»æ‰¾åˆ°åˆé€‚çš„ç­”æ¡ˆï¼Œå› æ­¤ï¼Œå¤§æ¨¡å‹ç»™å‡ºäº†ä¸€ä¸ªæ­£ç¡®çš„å›ç­”ã€‚ æ³¨æ„: çŸ¥è¯†åº“çš„æœç´¢æƒ…å†µå–å†³äºåµŒå…¥æ¨¡å‹çš„å‡†åº¦ï¼Œåˆ†è¯å™¨çš„è®¾ç½®ï¼ŒçŸ¥è¯†åº“çš„æ’ç‰ˆå’Œå¤§æ¨¡å‹çš„æ•°é‡ï¼Œæç¤ºè¯è®¾å®šç­‰å¤šä¸ªå› ç´ ã€‚å› æ­¤ï¼Œéœ€è¦å¼€å‘è€…è¿›è¡Œæ·±åº¦çš„ä¼˜åŒ–å’Œè°ƒè¯•ã€‚ æ£€æŸ¥æ˜¯å¦æˆåŠŸè°ƒç”¨Agentå·¥å…· è‹¥æˆåŠŸè°ƒç”¨Agentå·¥å…·ï¼Œåˆ™ä½ åº”è¯¥çœ‹åˆ°å¤§æ¨¡å‹å®Œæ•´çš„æ€ç»´è¿‡ç¨‹ï¼Œè¿™ä¼šåœ¨æ€è€ƒè¿‡ç¨‹ä¸‹æ‹‰æ¡†ä¸­æ˜¾ç¤ºå‡ºæ¥ã€‚å¦‚æœæˆåŠŸè°ƒç”¨Agentå·¥å…·ï¼Œåˆ™ä½ åº”è¯¥çœ‹åˆ°Markdownå¼•ç”¨æ•ˆæœçš„å·¥å…·ä½¿ç”¨æƒ…å†µã€‚ åœ¨Agentå¯¹è¯æ¨¡å¼ä¸­ï¼Œæ€è€ƒè¿‡ç¨‹ä¸­æ˜¾ç¤ºçš„æ˜¯å¤§æ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹ï¼Œè€Œä¸‹æ‹‰æ¡†ä¹‹å‰çš„å†…å®¹ä¸ºå¤§æ¨¡å‹çš„Final Answerï¼Œç¼ºä¹ä¸­é—´çš„è¿ç®—è¿‡ç¨‹ã€‚ ä¸‹å›¾å±•ç°äº†ä¸€ä¸ªæˆåŠŸè°ƒç”¨Agentå·¥å…·çš„æ•ˆæœå›¾: æœ¬æ¡†æ¶æ”¯æŒæ¨¡å‹è¿ç»­æ‰ç”¨å¤šä¸ªAgentå·¥å…·ï¼Œä¸‹å›¾å±•ç¤ºäº†ä¸€ä¸ªä¸€ä¸ªæé—®ä¸­å¤§æ¨¡å‹è¿ç»­è°ƒç”¨å¤šä¸ªAgentå·¥å…·çš„æ•ˆæœå›¾: åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼Œ3900æ˜¯å¤§æ¨¡å‹çš„æœ€ç»ˆç­”æ¡ˆï¼Œå…¶ä½™éƒ½æ˜¯æ€è€ƒè¿‡ç¨‹ã€‚ Â© snowdreams1006 all right reservedï¼Œpowered by Gitbookæ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š 2024-05-17 23:23:32 "},"LLM/åšå‡ºè´¡çŒ®.html":{"url":"LLM/åšå‡ºè´¡çŒ®.html","title":"åšå‡ºè´¡çŒ®","keywords":"","body":"Issue è§„èŒƒ ä»€ä¹ˆæ ·çš„ issue æ˜¯ä¸ä¼šè¢«å›å¤çš„ åœ¨æå‡ºissueå‰ï¼Œè¯·æŸ¥çœ‹æ‚¨çš„æå‡ºçš„é—®é¢˜æ˜¯å¦å·²ç»åœ¨ issue åˆ—è¡¨æˆ–è€… discussion å†…å‡ºç°ï¼Œæå‡ºé‡å¤çš„é—®é¢˜å°† è¢«å…³é—­ ã€‚ éé¡¹ç›®æ¨èé…ç½®çš„ä»»ä½•å…³äºç¯å¢ƒé…ç½®é—®é¢˜çš„ issue é€šå¸¸å°† ä¸ä¼šç”±å®˜æ–¹å›å¤ï¼Œè¯·æ‚¨åœ¨å¾®ä¿¡æ²Ÿé€šç¾¤å†…å’¨è¯¢ã€‚ ä¸é¡¹ç›®æ— å…³çš„ issue å°† ä¸ä¼šè¢«å›å¤ ã€‚ è¶…è¿‡30å¤©æ²¡æœ‰æ›´æ–°åŠ¨æ€çš„ issue å°† è¢«å…³é—­ ã€‚ è¯­è¨€éä¸­æ–‡å’Œè‹±è¯­çš„ issue å°† è¢«å…³é—­ ã€‚ æ²¡æœ‰å°è¯•è¿‡è§£å†³æ–¹æ¡ˆçš„ issue å°† è¢«å…³é—­ ã€‚ æ²¡æœ‰æå‡ºä»»ä½•è´¡çŒ®ï¼ˆä¾‹å¦‚PRï¼Œè®ºæ–‡ï¼‰çš„ feature / enhancement å°†ä¼š è¢«å…³é—­ ã€‚æ‚¨å¯ä»¥åœ¨ discussion ä¸­çš„ å¸Œæœ›å¼€å‘çš„åŠŸèƒ½ è®¨è®ºåŒºä¸­ç•™è¨€ï¼Œæˆ‘ä»¬å¼€å‘ç»„ä¼šè¿›è¡Œå›å¤ã€‚ ä¸æŒ‰ç…§ Issue è§„èŒƒæå‡ºçš„ issue å¯èƒ½å°† è¢«å…³é—­ ã€‚ å¦‚ä½•æ issue ç®€è¦é˜è¿°ä½ çš„é—®é¢˜ é…ä¸ŠæŠ¥é”™æ—¥å¿—ä»¥(è¿è¡ŒæŠ¥é”™)æˆ–è€…è¿è¡Œä¸ç†æƒ³çš„æ•ˆæœå›¾(åŸæœ¬æœŸæœ›å’Œç°å®çš„) é…ä¸Šå¯¹åº”çš„é…ç½®æ–‡ä»¶ä»¥ä½ çš„ç¯å¢ƒ ä½ å°è¯•è¿‡çš„è§£å†³æ–¹æ³•ã€‚ï¼ˆéå¸¸é‡è¦ï¼‰ æŒ‰ç…§æ¨¡æ¿æå‡ºIssue PR è§„èŒƒ ä»€ä¹ˆæ ·çš„ PR æ˜¯ä¸ä¼šè¢«æ¥å—çš„ éç´§æ€¥bugä¿®å¤çš„PRå¹¶ç›´æ¥æäº¤åˆ°masterçš„PRã€‚ ä»…ä»…ä¿®æ”¹Readme.mdå’Œé…ç½®æ–‡ä»¶çš„ã€‚ è·Ÿé¡¹ç›®ç»„å·²ç»å¼€å‘çš„å†…å®¹å†²çªçš„(devç‰ˆæœ¬)ï¼Œå°†å¯èƒ½è¢«æ‹’ç»ã€‚ é¦–å…ˆè¯·æ³¨æ„æ‰€æœ‰çš„PRéœ€è¦ä»¥devåˆ†æ”¯ä¸ºåŸºå‡†ï¼Œmasteråˆ†æ”¯ä»…ç”¨æ¥å‘è¡Œä¸ç´§æ€¥bugä¿®å¤ã€‚ æå‡ºæ–°çš„é€šç”¨è‡ªå®šä¹‰åˆ†è¯å™¨ å°†æ‚¨çš„åˆ†è¯å™¨æ‰€åœ¨çš„ä»£ç æ–‡ä»¶æ”¾åœ¨text_splitteræ–‡ä»¶å¤¹ä¸‹ï¼Œæ–‡ä»¶åä¸ºæ‚¨çš„åˆ†è¯å™¨åå­—my_splitter.pyï¼Œç„¶ååœ¨__init__.pyä¸­å¯¼å…¥æ‚¨çš„åˆ†è¯å™¨ã€‚ å‘èµ·PRï¼Œå¹¶è¯´æ˜æ‚¨çš„åˆ†è¯å™¨é¢å‘çš„åœºæ™¯æˆ–è€…æ”¹è¿›ä¹‹å¤„ã€‚æˆ‘ä»¬éå¸¸æœŸå¾…æ‚¨èƒ½ä¸¾ä¾‹ä¸€ä¸ªå…·ä½“çš„åº”ç”¨åœºæ™¯ã€‚ æå‡ºæ–°çš„ Agent å·¥å…· å°†æ‚¨çš„Agentå·¥å…·æ‰€åœ¨çš„ä»£ç æ”¾åœ¨ server/agentæ–‡ä»¶å¤¹ä¸‹ï¼Œæ–‡ä»¶åä¸ºæ‚¨çš„å·¥å…·åå­—my_tools.pyï¼Œç„¶ååœ¨tools.pyä¸­å¯¼å…¥æ‚¨çš„å·¥å…·ã€‚ å‘èµ·PRï¼Œè¯´æ˜æ‚¨çš„å·¥å…·é¢å‘çš„åœºæ™¯æˆ–æ”¹è¿›ä¹‹å¤„ï¼Œå¹¶è¯´æ˜å¦‚ä½•è¿›è¡Œæµ‹è¯•å’Œè°ƒç”¨ã€‚æˆ‘ä»¬éå¸¸æœŸå¾…æ‚¨èƒ½ä¸¾ä¾‹ä¸€ä¸ªå…·ä½“çš„åº”ç”¨åœºæ™¯ã€‚ æå‡ºæ–°çš„è‡ªå®šä¹‰æ¨¡å‹ å°†æ‚¨çš„æ¨¡å‹è´¡çŒ®åˆ°huggingfaceå¹³å°ä¸Šï¼Œå¹¶å¼€æ”¾ç»™å¼€å‘äººå‘˜ä¸‹è½½ã€‚ å‘èµ·PRï¼Œè¯´æ˜æ‚¨çš„å·¥å…·é¢å‘çš„åœºæ™¯æˆ–æ”¹è¿›ä¹‹å¤„ï¼Œå¹¶è¯´æ˜å¦‚ä½•è¿›è¡Œæµ‹è¯•å’Œè°ƒç”¨ã€‚æˆ‘ä»¬éå¸¸æœŸå¾…æ‚¨èƒ½ä¸¾ä¾‹ä¸€ä¸ªå…·ä½“çš„åº”ç”¨åœºæ™¯ã€‚ ç”±å¼€å‘äººå‘˜æµ‹è¯•é€šè¿‡åï¼Œå°†æ‚¨çš„æ¨¡å‹æ·»åŠ åˆ°åˆä½œæ¨¡å‹åå•ä¸­ã€‚ ä¿®å¤ Bug & å¢åŠ å…¶ä»–æ–°åŠŸèƒ½ ä¸€ä¸ª PR ä¸­å¿…é¡» åªæœ‰ä¸€ä¸ªæˆ–è€…ä¸€ç±»åŠŸèƒ½å¢åŠ ï¼Œæˆ–è€…ä¿®å¤ä¸€ä¸ªbug ï¼Œå¤šä¸ªåŠŸèƒ½æ··åˆçš„ PR å°† ä¸ä¼šè¢«æ¥å— ã€‚ è¯´æ˜æ‚¨å¢åŠ çš„åŠŸèƒ½æˆ–è€…æ”¹è¿›ä¹‹å¤„ï¼Œå¹¶è¯´æ˜å¦‚ä½•è¿›è¡Œæµ‹è¯•å’Œè°ƒç”¨ã€‚æˆ‘ä»¬éå¸¸æœŸå¾…æ‚¨èƒ½ä¸¾ä¾‹ä¸€ä¸ªå…·ä½“çš„åº”ç”¨åœºæ™¯ã€‚ Â© snowdreams1006 all right reservedï¼Œpowered by Gitbookæ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š 2024-05-17 23:23:32 "},"LLM/å¸¸è§é—®é¢˜.html":{"url":"LLM/å¸¸è§é—®é¢˜.html","title":"å¸¸è§é—®é¢˜","keywords":"","body":" ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„é—®é¢˜å’Œå›ç­” Q: æˆ‘è¦æå‡ºé—®é¢˜ï¼Œæ€ä¹ˆåŠ A: é¦–å…ˆï¼Œä½ è¦è§‚å¯Ÿä¸€ä¸‹ä½ çš„é—®é¢˜æ˜¯å¦æœ‰æ²¡æœ‰è¢«è§£å†³ï¼Œå»ºè®®ç¿»çœ‹ä»¥å¾€çš„Issueå’ŒDiscussionï¼Œå¦‚æœæœ‰ï¼Œå…ˆæŒ‰ç…§ä»–ä»¬çš„æ–¹æ³•æ¥åšã€‚ å¦‚æœæ²¡æœ‰ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ è¿™æ˜¯ä¸€ä¸ªbugè¿˜æ˜¯ä¸€ä¸ªè®¨è®ºé—®é¢˜ï¼Œå¦‚æœæ˜¯è®¨è®ºé—®é¢˜ï¼Œæ”¾åœ¨disscusionï¼Œå¦‚æœæ˜¯bugå’Œfeatureï¼Œæ”¾åœ¨issueã€‚ å¦‚æœè¦æå‡ºfeatureï¼Œæäº¤ä¸€ä»½å¯¹åº”çš„PRä¼šè®©å¼€å‘è€…æ›´é‡è§†ä½ çš„é—®é¢˜ï¼Œå¦åˆ™ä½ çš„é—®é¢˜å¾ˆæœ‰å¯èƒ½è¢«ç›´æ¥å…³é—­ã€‚ Q: ValueError: Found modules on cpu/disk. Using Exllama backend requires all the modules to be on GPU. You can deactivate exllama backend by setting disable_exllama=True in the quantization config object. A: è¿™æ˜¯Fschatä¾èµ–æºç çš„é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹è§£å†³æ–¹å¼ï¼Œé€šè¿‡ä¿®æ”¹'Fschat'åº“ä¸­çš„å¯¹åº”å†…å®¹ã€‚ https://github.com/lm-sys/FastChat/issues/2459 https://stackoverflow.com/questions/76983305/fine-tuning-thebloke-llama-2-13b-chat-gptq-model-with-hugging-face-transformers Q: AttributeError: 'ChatGLMTokenizer' object has no attribute 'tokenizer' A: æŸ¥çœ‹ä»¥ä¸‹Issue https://github.com/chatchat-space/Langchain-Chatchat/issues/1835 Q: ä½¿ç”¨Qwen API key æŠ¥é”™ multiple wodgets with the same keyï¼â€œ A: ç¡®ä¿ä½ çš„keyæ˜¯dashscopeå¹³å°çš„keyã€‚å¹¶ä¿è¯dashscopeä¾èµ–æ»¡è¶³æˆ‘ä»¬çš„ä¾èµ–ç‰ˆæœ¬ã€‚ Qï¼šlinuxä¸‹å‘é‡åŒ–PDFæ–‡ä»¶æ—¶å‡ºé”™ï¼šImportError: ä»æ–‡ä»¶ *.pdf åŠ è½½æ–‡æ¡£æ—¶å‡ºé”™ï¼šlibGL.so.1: cannot open shared object file: No such file or directory Aï¼š è¿™æ˜¯ç³»ç»Ÿç¼ºå°‘å¿…è¦çš„åŠ¨æ€åº“ï¼Œå¯ä»¥æ‰‹åŠ¨å®‰è£…ï¼šlibgl1-mesa-glx å’Œ libglib2.0-0 Q: å„ç§Int4æ¨¡å‹æ— æ³•è½½å…¥ A. ç”±äºå„ç§Int4æ¨¡å‹ä¸Fp16æ¨¡å‹å¹¶ä¸ç›¸ä¼¼ï¼Œä¸”é‡åŒ–æŠ€æœ¯å¯èƒ½æœ‰æ‰€ä¸åŒï¼Œæ— æ³•è½½å…¥å¯èƒ½æ˜¯å› ä¸ºfschatä¸æ”¯æŒæˆ–è€…ç¼ºå°‘å¯¹åº”çš„ä¾èµ–ï¼Œéœ€è¦æŸ¥çœ‹å¯¹åº”ä»“åº“çš„issueè·å¾—æ›´å¤šä¿¡æ¯ã€‚å¼€å‘ç»„æ²¡æœ‰é’ˆå¯¹Int4æ¨¡å‹è¿›è¡Œä¼˜åŒ–ã€‚ Q1: æœ¬é¡¹ç›®æ”¯æŒå“ªäº›æ–‡ä»¶æ ¼å¼ï¼Ÿ A1: ç›®å‰å·²æµ‹è¯•æ”¯æŒ txtã€docxã€mdã€pdfã€csvã€htmlã€json ç­‰æ ¼å¼æ–‡ä»¶ æ›´å¤šæ–‡ä»¶æ ¼å¼è¯·å‚è€ƒ langchain æ–‡æ¡£ã€‚ç›®å‰å·²çŸ¥æ–‡æ¡£ä¸­è‹¥å«æœ‰ç‰¹æ®Šå­—ç¬¦ï¼Œå¯èƒ½å­˜åœ¨æ–‡ä»¶æ— æ³•åŠ è½½çš„é—®é¢˜ã€‚ Q2: ä½¿ç”¨è¿‡ç¨‹ä¸­ Python åŒ… nltkå‘ç”Ÿäº† Resource punkt not found.æŠ¥é”™ï¼Œè¯¥å¦‚ä½•è§£å†³ï¼Ÿ A2: æ–¹æ³•ä¸€ï¼šhttps://github.com/nltk/nltk_data/raw/gh-pages/packages/tokenizers/punkt.zip ä¸­çš„ packages/tokenizers è§£å‹ï¼Œæ”¾åˆ° nltk_data/tokenizers å­˜å‚¨è·¯å¾„ä¸‹ã€‚ nltk_data å­˜å‚¨è·¯å¾„å¯ä»¥é€šè¿‡ nltk.data.path æŸ¥è¯¢ã€‚ æ–¹æ³•äºŒï¼šæ‰§è¡Œpythonä»£ç  import nltk nltk.download() Q3: ä½¿ç”¨è¿‡ç¨‹ä¸­ Python åŒ… nltkå‘ç”Ÿäº† Resource averaged_perceptron_tagger not found.æŠ¥é”™ï¼Œè¯¥å¦‚ä½•è§£å†³ï¼Ÿ A3: æ–¹æ³•ä¸€ï¼šå°† https://github.com/nltk/nltk_data/blob/gh-pages/packages/taggers/averaged_perceptron_tagger.zip ä¸‹è½½ï¼Œè§£å‹æ”¾åˆ° nltk_data/taggers å­˜å‚¨è·¯å¾„ä¸‹ã€‚ nltk_data å­˜å‚¨è·¯å¾„å¯ä»¥é€šè¿‡ nltk.data.path æŸ¥è¯¢ã€‚ æ–¹æ³•äºŒï¼šæ‰§è¡Œpythonä»£ç  import nltk nltk.download() Q4: æœ¬é¡¹ç›®å¯å¦åœ¨ colab ä¸­è¿è¡Œï¼Ÿ A4: å¯ä»¥å°è¯•ä½¿ç”¨ chatglm-6b-int4 æ¨¡å‹åœ¨ colab ä¸­è¿è¡Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚éœ€åœ¨ colab ä¸­è¿è¡Œ Web UIï¼Œéœ€å°† webui.pyä¸­ demo.queue(concurrency_count=3).launch( server_name='0.0.0.0', share=False, inbrowser=False)ä¸­å‚æ•° shareè®¾ç½®ä¸º Trueã€‚ Q5: åœ¨ Anaconda ä¸­ä½¿ç”¨ pip å®‰è£…åŒ…æ— æ•ˆå¦‚ä½•è§£å†³ï¼Ÿ A5: æ­¤é—®é¢˜æ˜¯ç³»ç»Ÿç¯å¢ƒé—®é¢˜ï¼Œè¯¦ç»†è§ åœ¨Anacondaä¸­ä½¿ç”¨pipå®‰è£…åŒ…æ— æ•ˆé—®é¢˜ Q6: æœ¬é¡¹ç›®ä¸­æ‰€éœ€æ¨¡å‹å¦‚ä½•ä¸‹è½½è‡³æœ¬åœ°ï¼Ÿ A6: æœ¬é¡¹ç›®ä¸­ä½¿ç”¨çš„æ¨¡å‹å‡ä¸º huggingface.com ä¸­å¯ä¸‹è½½çš„å¼€æºæ¨¡å‹ï¼Œä»¥é»˜è®¤é€‰æ‹©çš„ chatglm-6bå’Œ text2vec-large-chineseæ¨¡å‹ä¸ºä¾‹ï¼Œä¸‹è½½æ¨¡å‹å¯æ‰§è¡Œå¦‚ä¸‹ä»£ç ï¼š # å®‰è£… git lfs $ git lfs install # ä¸‹è½½ LLM æ¨¡å‹ $ git clone https://huggingface.co/THUDM/chatglm-6b /your_path/chatglm-6b # ä¸‹è½½ Embedding æ¨¡å‹ $ git clone https://huggingface.co/GanymedeNil/text2vec-large-chinese /your_path/text2vec # æ¨¡å‹éœ€è¦æ›´æ–°æ—¶ï¼Œå¯æ‰“å¼€æ¨¡å‹æ‰€åœ¨æ–‡ä»¶å¤¹åæ‹‰å–æœ€æ–°æ¨¡å‹æ–‡ä»¶/ä»£ç  $ git pull Q7: huggingface.comä¸­æ¨¡å‹ä¸‹è½½é€Ÿåº¦è¾ƒæ…¢æ€ä¹ˆåŠï¼Ÿ A7: å¯ä½¿ç”¨æœ¬é¡¹ç›®ç”¨åˆ°çš„æ¨¡å‹æƒé‡æ–‡ä»¶ç™¾åº¦ç½‘ç›˜åœ°å€ï¼š ernie-3.0-base-zh.zip é“¾æ¥: https://pan.baidu.com/s/1CIvKnD3qzE-orFouA8qvNQ?pwd=4wih ernie-3.0-nano-zh.zip é“¾æ¥: https://pan.baidu.com/s/1Fh8fgzVdavf5P1omAJJ-Zw?pwd=q6s5 text2vec-large-chinese.zip é“¾æ¥: https://pan.baidu.com/s/1sMyPzBIXdEzHygftEoyBuA?pwd=4xs7 chatglm-6b-int4-qe.zip é“¾æ¥: https://pan.baidu.com/s/1DDKMOMHtNZccOOBGWIOYww?pwd=22ji chatglm-6b-int4.zip é“¾æ¥: https://pan.baidu.com/s/1pvZ6pMzovjhkA6uPcRLuJA?pwd=3gjd chatglm-6b.zip é“¾æ¥: https://pan.baidu.com/s/1B-MpsVVs1GHhteVBetaquw?pwd=djay Q8: è€ç‰ˆæœ¬å’Œæ–°ç‰ˆæœ¬æ— æ³•å…¼å®¹æ€ä¹ˆåŠï¼Ÿ A8: ä¿å­˜è€ç‰ˆæœ¬çš„é…ç½®æ–‡ä»¶ï¼Œåˆ é™¤è€ç‰ˆæœ¬ä»£ç å¹¶ä¸‹è½½æ–°ç‰ˆæœ¬ä»£ç åï¼Œæ ¹æ®æ–°ç‰ˆæœ¬çš„é…ç½®æ–‡ä»¶æ ¼å¼è¿›è¡Œä¿®æ”¹ã€‚ åœ¨ 0.2.6åï¼Œè¿è¡Œç¯å¢ƒå’Œé…ç½®æ–‡ä»¶å‘ç”Ÿé‡å¤§å˜åŒ–ï¼Œå»ºè®®é‡æ–°é…ç½®ç¯å¢ƒå’Œé…ç½®æ–‡ä»¶ï¼Œå¹¶é‡å»ºçŸ¥è¯†åº“ã€‚ Q9: æ˜¾å¡å†…å­˜çˆ†äº†ï¼Œæç¤º \"OutOfMemoryError: CUDA out of memory\" A9: VECTOR_SEARCH_TOP_K å’Œ HISTORY_LEN çš„å€¼è°ƒä½ï¼Œæ¯”å¦‚ VECTOR_SEARCH_TOP_K = 3 å’Œ LLM_HISTORY_LEN = 2ï¼Œè¿™æ ·ç”± query å’Œ context æ‹¼æ¥å¾—åˆ°çš„ prompt ä¼šå˜çŸ­ï¼Œä¼šå‡å°‘å†…å­˜çš„å ç”¨ã€‚æˆ–è€…ä½¿ç”¨é‡åŒ–æ¨¡å‹å‡å°‘æ˜¾å­˜å ç”¨ã€‚ Q10: æ‰§è¡Œ pip install -r requirements.txt è¿‡ç¨‹ä¸­é‡åˆ° python åŒ…ï¼Œå¦‚ langchain æ‰¾ä¸åˆ°å¯¹åº”ç‰ˆæœ¬çš„é—®é¢˜ A10: æ›´æ¢ pypi æºåé‡æ–°å®‰è£…ï¼Œå¦‚é˜¿é‡Œæºã€æ¸…åæºç­‰ï¼Œç½‘ç»œæ¡ä»¶å…è®¸æ—¶å»ºè®®ç›´æ¥ä½¿ç”¨ pypi.org æºï¼Œå…·ä½“æ“ä½œå‘½ä»¤å¦‚ä¸‹ï¼š # ä½¿ç”¨ pypi æº $ pip install -r requirements.txt -i https://pypi.python.org/simple æˆ– # ä½¿ç”¨é˜¿é‡Œæº $ pip install -r requirements.txt -i http://mirrors.aliyun.com/pypi/simple/ æˆ– # ä½¿ç”¨æ¸…åæº $ pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple/ Q11: å¯åŠ¨ api.py æ—¶ upload_file æ¥å£æŠ›å‡º partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import) A11: è¿™æ˜¯ç”±äº charset_normalizer æ¨¡å—ç‰ˆæœ¬è¿‡é«˜å¯¼è‡´çš„ï¼Œéœ€è¦é™ä½ä½ charset_normalizer çš„ç‰ˆæœ¬,æµ‹è¯•åœ¨ charset_normalizer==2.1.0 ä¸Šå¯ç”¨ã€‚ Q12: è°ƒç”¨apiä¸­çš„ bing_search_chat æ¥å£æ—¶ï¼ŒæŠ¥å‡º Failed to establish a new connection: [Errno 110] Connection timed out A12: è¿™æ˜¯å› ä¸ºæœåŠ¡å™¨åŠ äº†é˜²ç«å¢™ï¼Œéœ€è¦è”ç³»ç®¡ç†å‘˜åŠ ç™½åå•ï¼Œå¦‚æœå…¬å¸çš„æœåŠ¡å™¨çš„è¯ï¼Œå°±åˆ«æƒ³äº†GG--! Q13: åŠ è½½ chatglm-6b-int8 æˆ– chatglm-6b-int4 æŠ›å‡º RuntimeError: Only Tensors of floating point andcomplex dtype can require gradients A13: ç–‘ä¸º chatglm çš„ quantization çš„é—®é¢˜æˆ– torch ç‰ˆæœ¬å·®å¼‚é—®é¢˜ï¼Œé’ˆå¯¹å·²ç»å˜ä¸º Parameter çš„ torch.zeros çŸ©é˜µä¹Ÿæ‰§è¡Œ Parameter æ“ä½œï¼Œä»è€ŒæŠ›å‡º RuntimeError: Only Tensors of floating point andcomplex dtype can require gradientsã€‚è§£å†³åŠæ³•æ˜¯åœ¨ chatglm é¡¹ç›®çš„åŸå§‹æ–‡ä»¶ä¸­çš„ quantization.py æ–‡ä»¶ 374 è¡Œæ”¹ä¸ºï¼š try: self.weight =Parameter(self.weight.to(kwargs[\"device\"]), requires_grad=False) except Exception as e: pass å¦‚æœä¸Šè¿°æ–¹å¼ä¸èµ·ä½œç”¨ï¼Œåˆ™åœ¨.cache/hugggingface/modules/ç›®å½•ä¸‹é’ˆå¯¹chatglmé¡¹ç›®çš„åŸå§‹æ–‡ä»¶ä¸­çš„quantization.pyæ–‡ä»¶æ‰§è¡Œä¸Šè¿°æ“ä½œï¼Œè‹¥è½¯é“¾æ¥ä¸æ­¢ä¸€ä¸ªï¼ŒæŒ‰ç…§é”™è¯¯æç¤ºé€‰æ‹©æ­£ç¡®çš„è·¯å¾„ã€‚ æ³¨ï¼šè™½ç„¶æ¨¡å‹å¯ä»¥é¡ºåˆ©åŠ è½½ä½†åœ¨cpuä¸Šä»å­˜åœ¨æ¨ç†å¤±è´¥çš„å¯èƒ½ï¼šå³é’ˆå¯¹æ¯ä¸ªé—®é¢˜ï¼Œæ¨¡å‹ä¸€ç›´è¾“å‡ºguguguguã€‚ å› æ­¤ï¼Œæœ€å¥½ä¸è¦è¯•å›¾ç”¨cpuåŠ è½½é‡åŒ–æ¨¡å‹ï¼ŒåŸå› å¯èƒ½æ˜¯ç›®å‰pythonä¸»æµé‡åŒ–åŒ…çš„é‡åŒ–æ“ä½œæ˜¯åœ¨gpuä¸Šæ‰§è¡Œçš„,ä¼šå¤©ç„¶åœ°å­˜åœ¨gapã€‚ Q14: ä¿®æ”¹é…ç½®ä¸­è·¯å¾„åï¼ŒåŠ è½½ text2vec-large-chinese ä¾ç„¶æç¤º WARNING: No sentence-transformers model found with name text2vec-large-chinese. Creating a new one with MEAN pooling. A14: å°è¯•æ›´æ¢ embeddingï¼Œå¦‚ text2vec-base-chineseï¼Œè¯·åœ¨ configs/model_config.py æ–‡ä»¶ä¸­ï¼Œä¿®æ”¹ text2vec-baseå‚æ•°ä¸ºæœ¬åœ°è·¯å¾„ï¼Œç»å¯¹è·¯å¾„æˆ–è€…ç›¸å¯¹è·¯å¾„å‡å¯ Q16: ä½¿ç”¨pgå‘é‡åº“å»ºè¡¨æŠ¥é”™ A15: éœ€è¦æ‰‹åŠ¨å®‰è£…å¯¹åº”çš„vectoræ‰©å±•(è¿æ¥pgæ‰§è¡Œ CREATE EXTENSION IF NOT EXISTS vector) Q16: pymilvus è¿æ¥è¶…æ—¶ A16.pymilvusç‰ˆæœ¬éœ€è¦åŒ¹é…å’Œmilvuså¯¹åº”å¦åˆ™ä¼šè¶…æ—¶å‚è€ƒpymilvus==2.1.3 Q17: ä½¿ç”¨vllmæ¨ç†åŠ é€Ÿæ¡†æ¶æ—¶ï¼Œå·²ç»ä¸‹è½½äº†æ¨¡å‹ä½†å‡ºç°HuggingFaceé€šä¿¡é—®é¢˜ A17: å‚ç…§å¦‚ä¸‹ä»£ç ä¿®æ”¹pythonç¯å¢ƒä¸‹/site-packages/vllm/model_executor/weight_utils.pyæ–‡ä»¶çš„prepare_hf_model_weightså‡½æ•°å¦‚ä¸‹å¯¹åº”ä»£ç ï¼š if not is_local: # Use file lock to prevent multiple processes from # downloading the same model weights at the same time. model_path_temp = os.path.join( os.getenv(\"HOME\"), \".cache/huggingface/hub\", \"models--\" + model_name_or_path.replace(\"/\", \"--\"), \"snapshots/\", ) downloaded = False if os.path.exists(model_path_temp): temp_last_dir = os.listdir(model_path_temp)[-1] model_path_temp = os.path.join(model_path_temp, temp_last_dir) base_pattern = os.path.join(model_path_temp, \"pytorch_model*.bin\") files = glob.glob(base_pattern) if len(files) > 0: downloaded = True if downloaded: hf_folder = model_path_temp else: with get_lock(model_name_or_path, cache_dir): hf_folder = snapshot_download(model_name_or_path, allow_patterns=allow_patterns, cache_dir=cache_dir, tqdm_class=Disabledtqdm) else: hf_folder = model_name_or_path Q18: /xxx/base_model_worer.py æŠ¥ assert r.status_code == 200 é”™è¯¯ Aï¼šè¿™ä¸ªé”™è¯¯æ˜¯æœ¬åœ°æ¨¡å‹è¿›ç¨‹æ³¨å†Œåˆ° fastchat controller å¤±è´¥äº†ã€‚ä¸€èˆ¬æœ‰ä¸¤ç§åŸå› ï¼š1ã€å¼€äº†ç³»ç»Ÿå…¨å±€ä»£ç†ï¼Œå…³é—­å³å¯ã€‚2ã€DEFAULT_BIND_HOST è®¾ä¸º'0.0.0.0'ï¼Œæ”¹æˆ'127.0.0.1' æˆ– æœ¬æœºå®é™… IP å³å¯ã€‚æˆ–è€…æ›´æ–°åˆ°æœ€æ–°ç‰ˆæœ¬ä»£ç ä¹Ÿå¯ä»¥è§£å†³ã€‚ Q19: ä½¿ç”¨vllmåç«¯åŠ é€Ÿï¼Œæ— è¿”å›ä¸”ä¸æŠ¥é”™ã€‚ A: fschat=0.2.33çš„vllm_workerè„šæœ¬ä»£ç æœ‰bug, å¦‚éœ€ä½¿ç”¨ï¼Œéœ€æºç ä¿®æ”¹fastchat.server.vllm_workerï¼Œå°†103è¡Œä¸­sampling_params = SamplingParamsçš„å‚æ•°stop=list(stop)ä¿®æ”¹ä¸ºstop= [i for i in stop if i!=\"\"] Q20: chatglm3-6bå¯¹è¯ä¸­å‡ºç°\"\"æ ‡ç­¾ï¼Œä¸”è‡ªé—®è‡ªç­”ã€‚ A20: chatglm3å®˜æ–¹ç›®å‰å·²ç»ä¿®å¤äº†chatglm3-6bçš„é—®é¢˜ï¼Œè‹¥ä½¿ç”¨çš„æ¨¡å‹ä¸ºchatglm3-6bï¼Œä»…éœ€æ›´æ–°chatglm3-6bæ¨¡å‹ä»£ç å³å¯;è¯·å‰å¾€ Huggingface ä¸‹è½½æœ€æ–°çš„æƒé‡ã€‚ å¹¶æ›´æ–°fschatç‰ˆæœ¬åˆ° 0.2.34ä»¥ä¸Šã€‚ Q21: ä¸ºä»€ä¹ˆå¯åŠ¨çš„æ—¶å€™ä¸€ç›´å‡ºç° \"device not in ['cuda', 'mps', 'cpu','xpu'], device = auto\" çš„è­¦å‘Š A21: è¿™æ˜¯å› ä¸ºä½ æ²¡æœ‰åœ¨å¯¹åº”çš„å¯åŠ¨é€‰é¡¹è®¾å®šè®¾å¤‡ï¼Œè¯·åœ¨model_config.pyä¸­è®¾å®š DEVICEï¼Œä¸è¿‡ï¼Œå°±ç®—ä¸è®¾å®šï¼Œautoä¹Ÿèƒ½æ­£å¸¸ä½¿ç”¨ Â© snowdreams1006 all right reservedï¼Œpowered by Gitbookæ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š 2024-05-17 23:23:32 "},"LLM/åˆä½œä¼™ä¼´.html":{"url":"LLM/åˆä½œä¼™ä¼´.html","title":"åˆä½œä¼™ä¼´","keywords":"","body":"åˆä½œä¼™ä¼´åå• ğŸ‰ Langchain-Chatchat é¡¹ç›®åˆä½œä¼™ä¼´ï¼Œæ„Ÿè°¢ä»¥ä¸‹åˆä½œä¼™ä¼´å¯¹æœ¬é¡¹ç›®çš„æ”¯æŒã€‚ ChatGLM: å›½å†…æœ€æ—©çš„å¼€æºä¸­æ–‡å¤§æ¨¡å‹ä¹‹ä¸€ AutoDL æä¾›å¼¹æ€§ã€å¥½ç”¨ã€çœé’±çš„äº‘GPUç§Ÿç”¨æœåŠ¡ã€‚ç¼ºæ˜¾å¡å°±ä¸Š AutoDL.com å…¨çƒäº‘è®¡ç®—é¢†å¯¼è€… æˆ‘ä»¬ç›¸ä¿¡é¢„æµ‹æœªæ¥çš„æœ€å¥½æ–¹å¼æ˜¯è‡ªå·±æ¥åˆ›é€ ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œç­‰ä½ ã€‚ Â© snowdreams1006 all right reservedï¼Œpowered by Gitbookæ–‡ä»¶ä¿®è®¢æ—¶é—´ï¼š 2024-05-17 23:23:32 "}}