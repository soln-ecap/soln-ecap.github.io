在开始参数配置之前，先执行以下脚本
```shell
python copy_config_example.py
```
该脚本将会将所有```config```目录下的配置文件样例复制一份到```config```目录下，方便开发者进行配置。
接着，开发者可以根据自己的需求，对配置文件进行修改。

## 基础配置项 basic_config.py
该配置基负责记录日志的格式和储存路径，通常不需要修改。

## 模型配置项 model_config.py
本文件包含本地LLM模型、本地Embeddings模型、在线LLM模型API的相关配置。

- 本地模型路径配置。建议将所有下载的模型放到一个统一的目录下，然后将`MODEL_ROOT_PATH`指定为该目录，只要模型目录名称符合下列情况之一（以text2vec为例），即可自动识别加载：
  - text2vec，即MODEL_PATH中的键
  - GanymedeNil/text2vec-large-chinese，即MODEL_PATH中的值
  - text2vec-large-chinese，即MODEL_PATH中的值的简写形式

- 在线模型API配置。在`ONLINE_LLM_MODEL`已经预先写好了所有支持的在线API服务，通常只需要把申请的API_KEY等填入即可。
有些在线API服务需要安装额外的依赖：
  - zhipu-api: zhipuai
  - fangzhou-api: volcengine>=1.0.106
  - qianfan-api: qianfan
  - qwen-api: dashscope

- HISTORY_LEN。历史对话轮数通常不建议设置超过10，因为这可能导致以下问题
  1. 显存占用过高：尤其是部分模型，本身就已经要占用满显存的情况下，保留太多历史，一次传入token太多，可能会爆显存。
  2. 速度处理很慢：还是因为一次传入了太多token，导致速度很慢。

- TEMPERATURE。通常不建议设置过高。
在Agent对话模式和知识库问答中，我们强烈建议将要其设置成0或者接近于0。

- Agent_MODEL = None
我们支持用户使用“模型接力赛”的用法，即：
选择的大模型仅能调用工具，但是在工具中表现较差，则这个工具作为 “模型调用工具”
如果用户设置了Agent_MODEL，则在 Agent 中，使用Agent_MODEL来执行任务，否则，使用LLM_MODEL


## 提示词配置项 prompt_config.py

提示词配置分为三个板块，分别对应三种聊天类型。
- llm_chat: 基础的对话提示词， 通常来说，直接是用户输入的内容，没有系统提示词。
- knowledge_base_chat: 与知识库对话的提示词，在模板中，我们为开发者设计了一个系统提示词，开发者可以自行更改。
- agent_chat: 与Agent对话的提示词，同样，我们为开发者设计了一个系统提示词，开发者可以自行更改。

prompt模板使用Jinja2语法，简单点就是用双大括号代替f-string的单大括号
请注意，本配置文件支持热加载，修改prompt模板后无需重启服务。

## 数据库配置 kb_config.py
请确认本地分词器路径是否已经填写，如：

```
text_splitter_dict = {
   "ChineseRecursiveTextSplitter": {
       "source":"huggingface",  # 选择tiktoken则使用openai的方法,不填写则默认为字符长度切割方法。
       "tokenizer_name_or_path":"", # 空格不填则默认使用大模型的分词器。 
    }
}
```
设置好的分词器需要再```TEXT_SPLITTER_NAME```中指定并应用。

在这里，通常使用```huggingface```的方法，并且，我们推荐使用大模型自带的分词器来完成任务。

请注意，使用```gpt2```分词器将要访问huggingface官网下载权重。

我们还支持使用```tiktoken``` 和传统的 按照长度分词的方式，开发者可以自行配置。

如果希望调用自己的分词器，请参考[最佳实践]部分。

```kbs_config```设置了使用的向量数据库，目前可以选择
- ```faiss```: 使用faiss数据库，需要安装faiss-gpu
- ```milvus```: 使用milvus数据库，需要安装milvus并进行端口配置
- ```pg```: 使用pg数据库，需要配置connection_uri

## 服务和端口配置项 server_config.py

通常，这个页面并不需要进行大量的修改，仅需确保对应的端口打开，并不互相冲突即可。

如果你是Linux系统推荐设置

```
DEFAULT_BIND_HOST ="0.0.0.0"
```
如果使用联网模型，则需要关注联网模型的端口。

这些模型必须是在model_config.MODEL_PATH或ONLINE_MODEL中正确配置的。

#在启动startup.py时，可用通过`--model-worker --model-name xxxx`指定模型，不指定则为LLM_MODEL


## 覆盖配置文件 或者配置 startup.py

在 ```server_config.py```中有以下配置文件被注释了

```
"gpus": None, # 使用的GPU，以str的格式指定，如"0,1"，如失效请使用CUDA_VISIBLE_DEVICES="0,1"等形式指定
"num_gpus": 1, # 使用GPU的数量
"max_gpu_memory":"20GiB", # 每个GPU占用的最大显存

 以下为model_worker非常用参数，可根据需要配置
"load_8bit": False, # 开启8bit量化
"cpu_offloading": None,
"gptq_ckpt": None,
"gptq_wbits": 16,
"gptq_groupsize": -1,
"gptq_act_order": False,
"awq_ckpt": None,
"awq_wbits": 16,
"awq_groupsize": -1,
"model_names": [LLM_MODEL],
"conv_template": None,
"limit_worker_concurrency": 5,
"stream_interval": 2,
"no_register": False,
"embed_in_truncate": False,

 以下为vllm_woker配置参数,注意使用vllm必须有gpu，仅在Linux测试通过

 tokenizer = model_path # 如果tokenizer与model_path不一致在此处添加
 'tokenizer_mode':'auto',
 'trust_remote_code':True,
 'download_dir':None,
 'load_format':'auto',
 'dtype':'auto',
 'seed':0,
 'worker_use_ray':False,
 'pipeline_parallel_size':1,
 'tensor_parallel_size':1,
 'block_size':16,
 'swap_space':4 , # GiB
 'gpu_memory_utilization':0.90,
 'max_num_batched_tokens':2560,
 'max_num_seqs':256,
 'disable_log_stats':False,
 'conv_template':None,
 'limit_worker_concurrency':5,
 'no_register':False,
 'num_gpus': 1
 'engine_use_ray': False,
 'disable_log_requests': False
```

在这些参数中，如果没有设置，则使用```startup.py```中的默认值，如果设置了，则使用设置的值。
因此，强烈建议开发不要在```startup.py```中进行配置，而应该在```server_config.py```中进行配置。避免配置文件覆盖。

## 选择使用的模型
在```model_config.py```完成模型配置后，还不能直接使用，需要在该文件下配置本地模型的运行方式或在线模型的API，例如
```
    "agentlm-7b": { # 使用default中的IP和端口
       "device": "cuda",
    },
    "zhipu-api": { # 请为每个要运行的在线API设置不同的端口
        "port": 21001,
    },
```
本地模型使用default中的IP和端口，在线模型可以自己选择端口